[2023-10-12 10:33:30,423] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-12 10:33:32,186] [WARNING] [runner.py:203:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2023-10-12 10:33:32,187] [INFO] [runner.py:570:main] cmd = /home/haihongzhao/anaconda3/envs/speed/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbNiwgN119 --master_addr=127.0.0.1 --master_port=27000 --enable_each_rank_log=None main_zhh.py --data_path local/jsonfile --data_output_path /home/haihongzhao/project/LORA/step1_supervised_finetuning/data_output_path/bz4 --data_split 10,0,0 --model_name_or_path /data2/zhh/llama-meta --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --max_seq_len 512 --learning_rate 2e-5 --weight_decay 0. --num_train_epochs 9 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --num_warmup_steps 0 --seed 1234 --gradient_checkpointing --zero_stage 3 --deepspeed --lora_dim 8 --only_optimize_lora --lora_module_name self_attn. --output_dir /home/haihongzhao/project/LORA/step1_supervised_finetuning/training_log_output/lora_8_attn
[2023-10-12 10:33:33,998] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-12 10:33:36,087] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [6, 7]}
[2023-10-12 10:33:36,087] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2023-10-12 10:33:36,087] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2023-10-12 10:33:36,087] [INFO] [launch.py:163:main] dist_world_size=2
[2023-10-12 10:33:36,087] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=6,7
[2023-10-12 10:33:38,167] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-12 10:33:38,205] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2023-10-12 10:33:41,136] [INFO] [comm.py:637:init_distributed] cdb=None
[2023-10-12 10:33:41,137] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2023-10-12 10:33:41,354] [INFO] [comm.py:637:init_distributed] cdb=None
loading from ... /data2/zhh/llama-meta
loading from ... /data2/zhh/llama-meta
[2023-10-12 10:33:45,528] [INFO] [partition_parameters.py:347:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.39s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:23<00:23, 23.75s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 14.67s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 14.48s/it]

Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 13.42s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:29<00:00, 14.97s/it]
pad token is </s>
0 pad token is </s>
0 {'input_ids': tensor([    1, 29871, 13866,   338,   385, 15278,   393, 16612,   263,  3414,
        29889, 14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,
         2009, 29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13,
        29470,   756,  5320,  8646, 29895,   583, 29892,  1023, 22754, 29890,
          913, 29879,   322,  3023, 22843,  5663,   347,   874, 29892,   599,
        12944, 29889,  2688,  3897,   758,  5138,   424,   322,  1269, 22843,
         5663,   347,   369,   750,  1023,   901,  2653,   567,  1135,  1269,
         8646,  3459, 29889,   960,   278,  8646, 29895,   583,   322, 22754,
        29890,   913, 29879,   750, 29871, 29941,  2653,   567,  1269, 29892,
         1128,  1784,   901,  2653,   567,  1135, 16157, 26361,   526,   727,
          297,  3001, 29973,    13,    13,  2277, 29937, 13291, 29901,  1670,
          526, 29871, 29945, 29974, 29906,   353,  3532, 29945, 29974, 29906,
        29922, 29955,  6778, 29955,  8646, 29895,   583,   322, 22754, 29890,
          913, 29879,    13, 29955,  8646, 29895,   583,   322, 22754, 29890,
          913, 29879,   750, 29871, 29941,  2653,   567,  1269,   363,   263,
         3001,   310, 29871, 29941, 29930, 29955,   353,  3532, 29955, 29930,
        29941, 29922, 29906, 29896,  6778, 29906, 29896,  2653,   567,    13,
         9760, 22843,  5663,   347,   369,   750, 29871, 29906,   901,  2653,
          567,  1269,  1135,   278,  8646, 29895,   583,  1058,   750, 29871,
        29941,  1269,   577,  1269, 22843,  5663,   347,   369,   750, 29871,
        29941, 29974, 29906,   353,  3532, 29941, 29974, 29906, 29922, 29945,
         6778, 29945,  2653,   567,    13, 29946, 22843,  5663,   347,   874,
          750, 29871, 29945,  2653,   567,  1269,   363,   263,  3001,   310,
        29871, 29906, 29900,  2653,   567,    13,  8439,   526,   263,  3001,
          310, 29871, 29906, 29896, 29974, 29906, 29900,   353,  3532, 29906,
        29896, 29974, 29906, 29900, 29922, 29946, 29896,  6778, 29946, 29896,
         2653,   567,    13,  8439,   526,   263,  3001,   310, 29871, 29945,
        29974, 29906, 29974, 29946,   353,  3532, 29945, 29974, 29906, 29974,
        29946, 29922, 29896, 29896,  6778, 29896, 29896, 16157, 26361,    13,
         8439,   526, 29871, 29946, 29896, 29899, 29896, 29896,   353,  3532,
        29946, 29896, 29899, 29896, 29896, 29922, 29941, 29900,  6778, 29941,
        29900,   901,  2653,   567,  1135, 16157, 26361,    13,  4136, 29871,
        29941, 29900, 29966, 29989,   355,   974,   726, 29989, 29958,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([    1, 29871, 13866,   338,   385, 15278,   393, 16612,   263,  3414,
        29889, 14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,
         2009, 29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13,
        29470,   756,  5320,  8646, 29895,   583, 29892,  1023, 22754, 29890,
          913, 29879,   322,  3023, 22843,  5663,   347,   874, 29892,   599,
        12944, 29889,  2688,  3897,   758,  5138,   424,   322,  1269, 22843,
         5663,   347,   369,   750,  1023,   901,  2653,   567,  1135,  1269,
         8646,  3459, 29889,   960,   278,  8646, 29895,   583,   322, 22754,
        29890,   913, 29879,   750, 29871, 29941,  2653,   567,  1269, 29892,
         1128,  1784,   901,  2653,   567,  1135, 16157, 26361,   526,   727,
          297,  3001, 29973,    13,    13,  2277, 29937, 13291, 29901,  1670,
          526, 29871, 29945, 29974, 29906,   353,  3532, 29945, 29974, 29906,
        29922, 29955,  6778, 29955,  8646, 29895,   583,   322, 22754, 29890,
          913, 29879,    13, 29955,  8646, 29895,   583,   322, 22754, 29890,
          913, 29879,   750, 29871, 29941,  2653,   567,  1269,   363,   263,
         3001,   310, 29871, 29941, 29930, 29955,   353,  3532, 29955, 29930,
        29941, 29922, 29906, 29896,  6778, 29906, 29896,  2653,   567,    13,
         9760, 22843,  5663,   347,   369,   750, 29871, 29906,   901,  2653,
          567,  1269,  1135,   278,  8646, 29895,   583,  1058,   750, 29871,
        29941,  1269,   577,  1269, 22843,  5663,   347,   369,   750, 29871,
        29941, 29974, 29906,   353,  3532, 29941, 29974, 29906, 29922, 29945,
         6778, 29945,  2653,   567,    13, 29946, 22843,  5663,   347,   874,
          750, 29871, 29945,  2653,   567,  1269,   363,   263,  3001,   310,
        29871, 29906, 29900,  2653,   567,    13,  8439,   526,   263,  3001,
          310, 29871, 29906, 29896, 29974, 29906, 29900,   353,  3532, 29906,
        29896, 29974, 29906, 29900, 29922, 29946, 29896,  6778, 29946, 29896,
         2653,   567,    13,  8439,   526,   263,  3001,   310, 29871, 29945,
        29974, 29906, 29974, 29946,   353,  3532, 29945, 29974, 29906, 29974,
        29946, 29922, 29896, 29896,  6778, 29896, 29896, 16157, 26361,    13,
         8439,   526, 29871, 29946, 29896, 29899, 29896, 29896,   353,  3532,
        29946, 29896, 29899, 29896, 29896, 29922, 29941, 29900,  6778, 29941,
        29900,   901,  2653,   567,  1135, 16157, 26361,    13,  4136, 29871,
        29941, 29900, 29966, 29989,   355,   974,   726, 29989, 29958,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2])}{'input_ids': tensor([    1, 29871, 13866,   338,   385, 15278,   393, 16612,   263,  3414,
        29889, 14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,
         2009, 29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13,
        29470,   756,  5320,  8646, 29895,   583, 29892,  1023, 22754, 29890,
          913, 29879,   322,  3023, 22843,  5663,   347,   874, 29892,   599,
        12944, 29889,  2688,  3897,   758,  5138,   424,   322,  1269, 22843,
         5663,   347,   369,   750,  1023,   901,  2653,   567,  1135,  1269,
         8646,  3459, 29889,   960,   278,  8646, 29895,   583,   322, 22754,
        29890,   913, 29879,   750, 29871, 29941,  2653,   567,  1269, 29892,
         1128,  1784,   901,  2653,   567,  1135, 16157, 26361,   526,   727,
          297,  3001, 29973,    13,    13,  2277, 29937, 13291, 29901,  1670,
          526, 29871, 29945, 29974, 29906,   353,  3532, 29945, 29974, 29906,
        29922, 29955,  6778, 29955,  8646, 29895,   583,   322, 22754, 29890,
          913, 29879,    13, 29955,  8646, 29895,   583,   322, 22754, 29890,
          913, 29879,   750, 29871, 29941,  2653,   567,  1269,   363,   263,
         3001,   310, 29871, 29941, 29930, 29955,   353,  3532, 29955, 29930,
        29941, 29922, 29906, 29896,  6778, 29906, 29896,  2653,   567,    13,
         9760, 22843,  5663,   347,   369,   750, 29871, 29906,   901,  2653,
          567,  1269,  1135,   278,  8646, 29895,   583,  1058,   750, 29871,
        29941,  1269,   577,  1269, 22843,  5663,   347,   369,   750, 29871,
        29941, 29974, 29906,   353,  3532, 29941, 29974, 29906, 29922, 29945,
         6778, 29945,  2653,   567,    13, 29946, 22843,  5663,   347,   874,
          750, 29871, 29945,  2653,   567,  1269,   363,   263,  3001,   310,
        29871, 29906, 29900,  2653,   567,    13,  8439,   526,   263,  3001,
          310, 29871, 29906, 29896, 29974, 29906, 29900,   353,  3532, 29906,
        29896, 29974, 29906, 29900, 29922, 29946, 29896,  6778, 29946, 29896,
         2653,   567,    13,  8439,   526,   263,  3001,   310, 29871, 29945,
        29974, 29906, 29974, 29946,   353,  3532, 29945, 29974, 29906, 29974,
        29946, 29922, 29896, 29896,  6778, 29896, 29896, 16157, 26361,    13,
         8439,   526, 29871, 29946, 29896, 29899, 29896, 29896,   353,  3532,
        29946, 29896, 29899, 29896, 29896, 29922, 29941, 29900,  6778, 29941,
        29900,   901,  2653,   567,  1135, 16157, 26361,    13,  4136, 29871,
        29941, 29900, 29966, 29989,   355,   974,   726, 29989, 29958,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([    1, 29871, 13866,   338,   385, 15278,   393, 16612,   263,  3414,
        29889, 14350,   263,  2933,   393,  7128,  2486,  1614,  2167,   278,
         2009, 29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13,
        29470,   756,  5320,  8646, 29895,   583, 29892,  1023, 22754, 29890,
          913, 29879,   322,  3023, 22843,  5663,   347,   874, 29892,   599,
        12944, 29889,  2688,  3897,   758,  5138,   424,   322,  1269, 22843,
         5663,   347,   369,   750,  1023,   901,  2653,   567,  1135,  1269,
         8646,  3459, 29889,   960,   278,  8646, 29895,   583,   322, 22754,
        29890,   913, 29879,   750, 29871, 29941,  2653,   567,  1269, 29892,
         1128,  1784,   901,  2653,   567,  1135, 16157, 26361,   526,   727,
          297,  3001, 29973,    13,    13,  2277, 29937, 13291, 29901,  1670,
          526, 29871, 29945, 29974, 29906,   353,  3532, 29945, 29974, 29906,
        29922, 29955,  6778, 29955,  8646, 29895,   583,   322, 22754, 29890,
          913, 29879,    13, 29955,  8646, 29895,   583,   322, 22754, 29890,
          913, 29879,   750, 29871, 29941,  2653,   567,  1269,   363,   263,
         3001,   310, 29871, 29941, 29930, 29955,   353,  3532, 29955, 29930,
        29941, 29922, 29906, 29896,  6778, 29906, 29896,  2653,   567,    13,
         9760, 22843,  5663,   347,   369,   750, 29871, 29906,   901,  2653,
          567,  1269,  1135,   278,  8646, 29895,   583,  1058,   750, 29871,
        29941,  1269,   577,  1269, 22843,  5663,   347,   369,   750, 29871,
        29941, 29974, 29906,   353,  3532, 29941, 29974, 29906, 29922, 29945,
         6778, 29945,  2653,   567,    13, 29946, 22843,  5663,   347,   874,
          750, 29871, 29945,  2653,   567,  1269,   363,   263,  3001,   310,
        29871, 29906, 29900,  2653,   567,    13,  8439,   526,   263,  3001,
          310, 29871, 29906, 29896, 29974, 29906, 29900,   353,  3532, 29906,
        29896, 29974, 29906, 29900, 29922, 29946, 29896,  6778, 29946, 29896,
         2653,   567,    13,  8439,   526,   263,  3001,   310, 29871, 29945,
        29974, 29906, 29974, 29946,   353,  3532, 29945, 29974, 29906, 29974,
        29946, 29922, 29896, 29896,  6778, 29896, 29896, 16157, 26361,    13,
         8439,   526, 29871, 29946, 29896, 29899, 29896, 29896,   353,  3532,
        29946, 29896, 29899, 29896, 29896, 29922, 29941, 29900,  6778, 29941,
        29900,   901,  2653,   567,  1135, 16157, 26361,    13,  4136, 29871,
        29941, 29900, 29966, 29989,   355,   974,   726, 29989, 29958,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,
            2,     2])}

Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Installed CUDA version 11.8 does not match the version torch was compiled with 11.7 but since the APIs are compatible, accepting this combination
Using /home/zinanzheng/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Using /home/zinanzheng/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
Detected CUDA files, patching ldflags
Emitting ninja build file /home/zinanzheng/.cache/torch_extensions/py310_cu117/fused_adam/build.ninja...
Building extension module fused_adam...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module fused_adam...
Time to load fused_adam op: 0.08630585670471191 seconds
Loading extension module fused_adam...
Time to load fused_adam op: 0.10239267349243164 seconds
[2023-10-12 10:34:17,403] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown
[2023-10-12 10:34:17,403] [INFO] [comm.py:662:init_distributed] Distributed backend already initialized
[2023-10-12 10:34:17,430] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2023-10-12 10:34:17,432] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2023-10-12 10:34:17,432] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2023-10-12 10:34:17,457] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2023-10-12 10:34:17,457] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2023-10-12 10:34:17,457] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2023-10-12 10:34:17,457] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer
[2023-10-12 10:34:17,603] [INFO] [utils.py:803:see_memory_usage] Stage 3 initialize beginning
[2023-10-12 10:34:17,603] [INFO] [utils.py:804:see_memory_usage] MA 6.37 GB         Max_MA 7.04 GB         CA 8.25 GB         Max_CA 8 GB 
[2023-10-12 10:34:17,604] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 252.99 GB, percent = 50.2%
[2023-10-12 10:34:17,606] [INFO] [stage3.py:126:__init__] Reduce bucket size 500,000,000
[2023-10-12 10:34:17,606] [INFO] [stage3.py:127:__init__] Prefetch bucket size 30000000
[2023-10-12 10:34:17,718] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2023-10-12 10:34:17,719] [INFO] [utils.py:804:see_memory_usage] MA 6.37 GB         Max_MA 6.37 GB         CA 8.25 GB         Max_CA 8 GB 
[2023-10-12 10:34:17,719] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 253.68 GB, percent = 50.4%
Parameter Offload: Total persistent parameters: 266240 in 65 params
[2023-10-12 10:34:17,933] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2023-10-12 10:34:17,934] [INFO] [utils.py:804:see_memory_usage] MA 6.36 GB         Max_MA 6.37 GB         CA 8.25 GB         Max_CA 8 GB 
[2023-10-12 10:34:17,934] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 252.83 GB, percent = 50.2%
[2023-10-12 10:34:18,074] [INFO] [utils.py:803:see_memory_usage] Before creating fp16 partitions
[2023-10-12 10:34:18,075] [INFO] [utils.py:804:see_memory_usage] MA 6.36 GB         Max_MA 6.36 GB         CA 8.25 GB         Max_CA 8 GB 
[2023-10-12 10:34:18,075] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 253.84 GB, percent = 50.4%
[2023-10-12 10:34:18,417] [INFO] [utils.py:803:see_memory_usage] After creating fp16 partitions: 1
[2023-10-12 10:34:18,417] [INFO] [utils.py:804:see_memory_usage] MA 6.36 GB         Max_MA 6.36 GB         CA 6.87 GB         Max_CA 8 GB 
[2023-10-12 10:34:18,418] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 253.81 GB, percent = 50.4%
[2023-10-12 10:34:18,531] [INFO] [utils.py:803:see_memory_usage] Before creating fp32 partitions
[2023-10-12 10:34:18,532] [INFO] [utils.py:804:see_memory_usage] MA 6.36 GB         Max_MA 6.36 GB         CA 6.87 GB         Max_CA 7 GB 
[2023-10-12 10:34:18,532] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 252.73 GB, percent = 50.2%
[2023-10-12 10:34:18,663] [INFO] [utils.py:803:see_memory_usage] After creating fp32 partitions
[2023-10-12 10:34:18,664] [INFO] [utils.py:804:see_memory_usage] MA 6.38 GB         Max_MA 6.39 GB         CA 6.87 GB         Max_CA 7 GB 
[2023-10-12 10:34:18,664] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 253.93 GB, percent = 50.4%
[2023-10-12 10:34:18,773] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states
[2023-10-12 10:34:18,774] [INFO] [utils.py:804:see_memory_usage] MA 6.38 GB         Max_MA 6.38 GB         CA 6.87 GB         Max_CA 7 GB 
[2023-10-12 10:34:18,774] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 252.69 GB, percent = 50.2%
[2023-10-12 10:34:18,906] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states
[2023-10-12 10:34:18,906] [INFO] [utils.py:804:see_memory_usage] MA 6.41 GB         Max_MA 6.42 GB         CA 6.87 GB         Max_CA 7 GB 
[2023-10-12 10:34:18,906] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 254.08 GB, percent = 50.4%
[2023-10-12 10:34:18,907] [INFO] [stage3.py:448:_setup_for_real_optimizer] optimizer state initialized
[2023-10-12 10:34:19,140] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer
[2023-10-12 10:34:19,140] [INFO] [utils.py:804:see_memory_usage] MA 7.35 GB         Max_MA 7.35 GB         CA 7.8 GB         Max_CA 8 GB 
[2023-10-12 10:34:19,141] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 252.98 GB, percent = 50.2%
[2023-10-12 10:34:19,141] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[2023-10-12 10:34:19,141] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2023-10-12 10:34:19,141] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f686c0cac80>
[2023-10-12 10:34:19,141] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0005], mom=[(0.9, 0.95)]
[2023-10-12 10:34:19,142] [INFO] [config.py:967:print] DeepSpeedEngine configuration:
[2023-10-12 10:34:19,142] [INFO] [config.py:971:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-10-12 10:34:19,142] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-10-12 10:34:19,142] [INFO] [config.py:971:print]   amp_enabled .................. False
[2023-10-12 10:34:19,142] [INFO] [config.py:971:print]   amp_params ................... False
[2023-10-12 10:34:19,142] [INFO] [config.py:971:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   bfloat16_enabled ............. False
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f686c23bd60>
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   communication_data_type ...... None
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   dataloader_drop_last ......... False
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   disable_allgather ............ False
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   dump_state ................... False
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 100, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   elasticity_enabled ........... False
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   fp16_auto_cast ............... False
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   fp16_enabled ................. True
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   global_rank .................. 0
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   grad_accum_dtype ............. None
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 4
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   gradient_clipping ............ 1.0
[2023-10-12 10:34:19,143] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   load_universal_checkpoint .... False
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   loss_scale ................... 0
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   memory_breakdown ............. False
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   mics_shard_size .............. -1
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='step1_tensorboard/ds_tensorboard_logs/', job_name='step1_model_tensorboard') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   optimizer_name ............... None
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   optimizer_params ............. None
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   pld_enabled .................. False
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   pld_params ................... False
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   prescale_gradients ........... False
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   scheduler_name ............... None
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   scheduler_params ............. None
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   sparse_attention ............. None
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   steps_per_print .............. 10
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   train_batch_size ............. 32
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  4
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   use_node_local_storage ....... False
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   weight_quantization_config ... None
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   world_size ................... 2
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=sys.maxsize max_live_parameters=30000000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   zero_enabled ................. True
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True
[2023-10-12 10:34:19,144] [INFO] [config.py:971:print]   zero_optimization_stage ...... 3
[2023-10-12 10:34:19,145] [INFO] [config.py:957:print_user_config]   json = {
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 4, 
    "steps_per_print": 10, 
    "zero_optimization": {
        "stage": 3, 
        "offload_param": {
            "device": "none"
        }, 
        "offload_optimizer": {
            "device": "none"
        }, 
        "stage3_param_persistence_threshold": 1.000000e+04, 
        "stage3_max_live_parameters": 3.000000e+07, 
        "stage3_prefetch_bucket_size": 3.000000e+07, 
        "memory_efficient_linear": false
    }, 
    "fp16": {
        "enabled": true, 
        "loss_scale_window": 100
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "wall_clock_breakdown": false, 
    "hybrid_engine": {
        "enabled": false, 
        "max_out_tokens": 512, 
        "inference_tp_size": 1, 
        "release_inference_cache": false, 
        "pin_parameters": true, 
        "tp_gather_partition_size": 8
    }, 
    "tensorboard": {
        "enabled": false, 
        "output_path": "step1_tensorboard/ds_tensorboard_logs/", 
        "job_name": "step1_model_tensorboard"
    }
}
***** Running training *****
***** Evaluating perplexity, Epoch 0/9 *****
ppl: (19.472797393798828, 289345280.0)
Beginning of Epoch 1/9, Total Micro Batches 935
Epoch: 0, Total Step: 1, Loss: 22.703125
Invalidate trace cache @ step 0: expected module 6, but got module 0
[2023-10-12 10:39:42,597] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1
[2023-10-12 10:40:05,929] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768
Epoch: 0, Total Step: 11, Loss: 19.484375
[2023-10-12 10:41:08,897] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384
Epoch: 0, Total Step: 21, Loss: 12.3984375
Epoch: 0, Total Step: 31, Loss: 4.28515625
[2023-10-12 10:42:18,631] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=3, lr=[0.0004999863703357237], mom=[(0.9, 0.95)]
[2023-10-12 10:42:18,632] [INFO] [timer.py:260:stop] epoch=0/micro_step=40/global_step=10, RunningAvgSamplesPerSec=1.9308432097772006, CurrSamplesPerSec=2.291110310788861, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 41, Loss: 1.052734375
Epoch: 0, Total Step: 51, Loss: 0.71923828125
Epoch: 0, Total Step: 61, Loss: 0.685546875
Epoch: 0, Total Step: 71, Loss: 0.56103515625
[2023-10-12 10:44:38,570] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=3, lr=[0.0004999196163740212], mom=[(0.9, 0.95)]
[2023-10-12 10:44:38,571] [INFO] [timer.py:260:stop] epoch=0/micro_step=80/global_step=20, RunningAvgSamplesPerSec=2.1141728939821305, CurrSamplesPerSec=2.261490713914594, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 81, Loss: 0.445556640625
Epoch: 0, Total Step: 91, Loss: 0.724609375
Epoch: 0, Total Step: 101, Loss: 0.5751953125
Epoch: 0, Total Step: 111, Loss: 0.499755859375
[2023-10-12 10:46:58,917] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=3, lr=[0.0004997972495428924], mom=[(0.9, 0.95)]
[2023-10-12 10:46:58,917] [INFO] [timer.py:260:stop] epoch=0/micro_step=120/global_step=30, RunningAvgSamplesPerSec=2.1709933498725817, CurrSamplesPerSec=2.3282694494843623, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 121, Loss: 0.444580078125
Epoch: 0, Total Step: 131, Loss: 0.423583984375
Epoch: 0, Total Step: 141, Loss: 0.397216796875
Epoch: 0, Total Step: 151, Loss: 0.372802734375
[2023-10-12 10:49:17,591] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=3, lr=[0.0004996192970717751], mom=[(0.9, 0.95)]
[2023-10-12 10:49:17,592] [INFO] [timer.py:260:stop] epoch=0/micro_step=160/global_step=40, RunningAvgSamplesPerSec=2.2056469061026585, CurrSamplesPerSec=2.281328691301003, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 161, Loss: 0.27978515625
Epoch: 0, Total Step: 171, Loss: 0.360595703125
Epoch: 0, Total Step: 181, Loss: 0.373046875
Epoch: 0, Total Step: 191, Loss: 0.331298828125
[2023-10-12 10:51:35,916] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=3, lr=[0.0004993857985591917], mom=[(0.9, 0.95)]
[2023-10-12 10:51:35,917] [INFO] [timer.py:260:stop] epoch=0/micro_step=200/global_step=50, RunningAvgSamplesPerSec=2.227504504987128, CurrSamplesPerSec=2.2979961842892647, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 201, Loss: 0.426025390625
Epoch: 0, Total Step: 211, Loss: 0.34423828125
Epoch: 0, Total Step: 221, Loss: 0.57763671875
Epoch: 0, Total Step: 231, Loss: 0.232666015625
[2023-10-12 10:53:54,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=3, lr=[0.0004990968059639378], mom=[(0.9, 0.95)]
[2023-10-12 10:53:54,843] [INFO] [timer.py:260:stop] epoch=0/micro_step=240/global_step=60, RunningAvgSamplesPerSec=2.2404587477694986, CurrSamplesPerSec=2.3074515873522485, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 241, Loss: 0.369873046875
Epoch: 0, Total Step: 251, Loss: 0.43701171875
Epoch: 0, Total Step: 261, Loss: 0.345703125
Epoch: 0, Total Step: 271, Loss: 0.35595703125
[2023-10-12 10:56:12,846] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=3, lr=[0.0004987523835935206], mom=[(0.9, 0.95)]
[2023-10-12 10:56:12,846] [INFO] [timer.py:260:stop] epoch=0/micro_step=280/global_step=70, RunningAvgSamplesPerSec=2.2518506684575375, CurrSamplesPerSec=2.298146570434188, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 281, Loss: 0.52197265625
Epoch: 0, Total Step: 291, Loss: 0.25048828125
Epoch: 0, Total Step: 301, Loss: 0.340087890625
Epoch: 0, Total Step: 311, Loss: 0.318603515625
[2023-10-12 10:59:00,069] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=3, lr=[0.0004983526080898481], mom=[(0.9, 0.95)]
[2023-10-12 10:59:00,070] [INFO] [timer.py:260:stop] epoch=0/micro_step=320/global_step=80, RunningAvgSamplesPerSec=2.2021989676715426, CurrSamplesPerSec=1.1222557551950025, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 321, Loss: 0.38037109375
Epoch: 0, Total Step: 331, Loss: 0.324951171875
Epoch: 0, Total Step: 341, Loss: 0.30029296875
Epoch: 0, Total Step: 351, Loss: 0.330322265625
[2023-10-12 11:01:45,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=3, lr=[0.0004978975684121755], mom=[(0.9, 0.95)]
[2023-10-12 11:01:45,155] [INFO] [timer.py:260:stop] epoch=0/micro_step=360/global_step=90, RunningAvgSamplesPerSec=2.168838777131164, CurrSamplesPerSec=2.360204208423311, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 361, Loss: 0.27490234375
Epoch: 0, Total Step: 371, Loss: 0.30859375
Epoch: 0, Total Step: 381, Loss: 0.36865234375
Epoch: 0, Total Step: 391, Loss: 0.363525390625
[2023-10-12 11:04:01,440] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=3, lr=[0.0004973873658173088], mom=[(0.9, 0.95)]
[2023-10-12 11:04:01,440] [INFO] [timer.py:260:stop] epoch=0/micro_step=400/global_step=100, RunningAvgSamplesPerSec=2.185987222212124, CurrSamplesPerSec=2.354705740309069, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 401, Loss: 0.34228515625
Epoch: 0, Total Step: 411, Loss: 0.38037109375
Epoch: 0, Total Step: 421, Loss: 0.37060546875
Epoch: 0, Total Step: 431, Loss: 0.33154296875
[2023-10-12 11:06:18,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=3, lr=[0.0004968221138370738], mom=[(0.9, 0.95)]
[2023-10-12 11:06:18,635] [INFO] [timer.py:260:stop] epoch=0/micro_step=440/global_step=110, RunningAvgSamplesPerSec=2.198899474245388, CurrSamplesPerSec=2.341185017955407, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 441, Loss: 0.26806640625
Epoch: 0, Total Step: 451, Loss: 0.2998046875
Epoch: 0, Total Step: 461, Loss: 0.38330078125
Epoch: 0, Total Step: 471, Loss: 0.25927734375
[2023-10-12 11:08:35,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=3, lr=[0.000496201938253052], mom=[(0.9, 0.95)]
[2023-10-12 11:08:35,545] [INFO] [timer.py:260:stop] epoch=0/micro_step=480/global_step=120, RunningAvgSamplesPerSec=2.210093183918371, CurrSamplesPerSec=2.331972726472708, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 481, Loss: 0.2261962890625
Epoch: 0, Total Step: 491, Loss: 0.361572265625
Epoch: 0, Total Step: 501, Loss: 0.284423828125
Epoch: 0, Total Step: 511, Loss: 0.458251953125
[2023-10-12 11:10:53,299] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=3, lr=[0.0004955269770685911], mom=[(0.9, 0.95)]
[2023-10-12 11:10:53,300] [INFO] [timer.py:260:stop] epoch=0/micro_step=520/global_step=130, RunningAvgSamplesPerSec=2.218612831296296, CurrSamplesPerSec=2.32236951728883, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 521, Loss: 0.1995849609375
Epoch: 0, Total Step: 531, Loss: 0.30126953125
Epoch: 0, Total Step: 541, Loss: 0.274658203125
Epoch: 0, Total Step: 551, Loss: 0.32568359375
[2023-10-12 11:13:11,220] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=3, lr=[0.0004947973804780968], mom=[(0.9, 0.95)]
[2023-10-12 11:13:11,220] [INFO] [timer.py:260:stop] epoch=0/micro_step=560/global_step=140, RunningAvgSamplesPerSec=2.2257612024433113, CurrSamplesPerSec=2.3216085619024365, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 561, Loss: 0.285400390625
Epoch: 0, Total Step: 571, Loss: 0.259765625
Epoch: 0, Total Step: 581, Loss: 0.3330078125
Epoch: 0, Total Step: 591, Loss: 0.35546875
[2023-10-12 11:15:28,172] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=3, lr=[0.0004940133108336104], mom=[(0.9, 0.95)]
[2023-10-12 11:15:28,173] [INFO] [timer.py:260:stop] epoch=0/micro_step=600/global_step=150, RunningAvgSamplesPerSec=2.2330011659581386, CurrSamplesPerSec=2.3170746415020713, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 601, Loss: 0.398193359375
Epoch: 0, Total Step: 611, Loss: 0.305419921875
Epoch: 0, Total Step: 621, Loss: 0.359375
Epoch: 0, Total Step: 631, Loss: 0.378173828125
[2023-10-12 11:18:20,648] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=3, lr=[0.0004931749426086824], mom=[(0.9, 0.95)]
[2023-10-12 11:18:20,652] [INFO] [timer.py:260:stop] epoch=0/micro_step=640/global_step=160, RunningAvgSamplesPerSec=2.204754348978461, CurrSamplesPerSec=1.3637552566354567, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 641, Loss: 0.242919921875
Epoch: 0, Total Step: 651, Loss: 0.45703125
Epoch: 0, Total Step: 661, Loss: 0.357666015625
Epoch: 0, Total Step: 671, Loss: 0.2288818359375
[2023-10-12 11:20:48,484] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=3, lr=[0.0004922824623595475], mom=[(0.9, 0.95)]
[2023-10-12 11:20:48,485] [INFO] [timer.py:260:stop] epoch=0/micro_step=680/global_step=170, RunningAvgSamplesPerSec=2.2024461061060414, CurrSamplesPerSec=2.3989344704282174, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 681, Loss: 0.322265625
Epoch: 0, Total Step: 691, Loss: 0.338134765625
Epoch: 0, Total Step: 701, Loss: 0.422607421875
Epoch: 0, Total Step: 711, Loss: 0.29150390625
[2023-10-12 11:23:02,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=3, lr=[0.0004913360686836117], mom=[(0.9, 0.95)]
[2023-10-12 11:23:02,869] [INFO] [timer.py:260:stop] epoch=0/micro_step=720/global_step=180, RunningAvgSamplesPerSec=2.2118490473577936, CurrSamplesPerSec=2.3957245214302185, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 721, Loss: 0.38330078125
Epoch: 0, Total Step: 731, Loss: 0.337646484375
Epoch: 0, Total Step: 741, Loss: 0.329345703125
Epoch: 0, Total Step: 751, Loss: 0.338623046875
[2023-10-12 11:25:19,666] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=3, lr=[0.0004903359721752603], mom=[(0.9, 0.95)]
[2023-10-12 11:25:19,667] [INFO] [timer.py:260:stop] epoch=0/micro_step=760/global_step=190, RunningAvgSamplesPerSec=2.218354321817191, CurrSamplesPerSec=2.341406420223451, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 761, Loss: 0.39111328125
Epoch: 0, Total Step: 771, Loss: 0.287353515625
Epoch: 0, Total Step: 781, Loss: 0.336181640625
Epoch: 0, Total Step: 791, Loss: 0.262451171875
[2023-10-12 11:27:35,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=3, lr=[0.000489282395378995], mom=[(0.9, 0.95)]
[2023-10-12 11:27:35,341] [INFO] [timer.py:260:stop] epoch=0/micro_step=800/global_step=200, RunningAvgSamplesPerSec=2.2251055352463958, CurrSamplesPerSec=2.3313010674437433, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 801, Loss: 0.450927734375
Epoch: 0, Total Step: 811, Loss: 0.220947265625
Epoch: 0, Total Step: 821, Loss: 0.33984375
Epoch: 0, Total Step: 831, Loss: 0.3115234375
[2023-10-12 11:29:52,516] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=3, lr=[0.00048817557273991336], mom=[(0.9, 0.95)]
[2023-10-12 11:29:52,516] [INFO] [timer.py:260:stop] epoch=0/micro_step=840/global_step=210, RunningAvgSamplesPerSec=2.230127089367739, CurrSamplesPerSec=2.3306680807658404, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 841, Loss: 0.285400390625
Epoch: 0, Total Step: 851, Loss: 0.2442626953125
Epoch: 0, Total Step: 861, Loss: 0.279296875
Epoch: 0, Total Step: 871, Loss: 0.284423828125
[2023-10-12 11:32:08,042] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=3, lr=[0.00048701575055153876], mom=[(0.9, 0.95)]
[2023-10-12 11:32:08,043] [INFO] [timer.py:260:stop] epoch=0/micro_step=880/global_step=220, RunningAvgSamplesPerSec=2.2358785916962964, CurrSamplesPerSec=2.4057039413758274, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 881, Loss: 0.27490234375
Epoch: 0, Total Step: 891, Loss: 0.189208984375
Epoch: 0, Total Step: 901, Loss: 0.3271484375
Epoch: 0, Total Step: 911, Loss: 0.363037109375
[2023-10-12 11:34:26,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=3, lr=[0.0004858031869010148], mom=[(0.9, 0.95)]
[2023-10-12 11:34:26,153] [INFO] [timer.py:260:stop] epoch=0/micro_step=920/global_step=230, RunningAvgSamplesPerSec=2.2393809235363986, CurrSamplesPerSec=2.3289476873784394, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 0, Total Step: 921, Loss: 0.1859130859375
Epoch: 0, Total Step: 931, Loss: 0.28955078125
***** Evaluating perplexity, Epoch 1/9 *****
Invalidate trace cache @ step 0: expected module 0, but got module 6
ppl: 1.381417989730835
eval loss: 0.32310640811920166
Beginning of Epoch 2/9, Total Micro Batches 935
Epoch: 1, Total Step: 936, Loss: 0.255126953125
Epoch: 1, Total Step: 946, Loss: 0.327392578125
Epoch: 1, Total Step: 956, Loss: 0.367919921875
[2023-10-12 11:42:21,003] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=3, lr=[0.0004845381516116748], mom=[(0.9, 0.95)]
[2023-10-12 11:42:21,003] [INFO] [timer.py:260:stop] epoch=1/micro_step=25/global_step=240, RunningAvgSamplesPerSec=2.227022464834729, CurrSamplesPerSec=2.2883844586432467, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 966, Loss: 0.349365234375
Epoch: 1, Total Step: 976, Loss: 0.5244140625
Epoch: 1, Total Step: 986, Loss: 0.296630859375
Epoch: 1, Total Step: 996, Loss: 0.343017578125
[2023-10-12 11:44:38,413] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=3, lr=[0.00048322092618300017], mom=[(0.9, 0.95)]
[2023-10-12 11:44:38,414] [INFO] [timer.py:260:stop] epoch=1/micro_step=65/global_step=250, RunningAvgSamplesPerSec=2.2310240031127186, CurrSamplesPerSec=2.3448246945701454, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1006, Loss: 0.27783203125
Epoch: 1, Total Step: 1016, Loss: 0.2369384765625
Epoch: 1, Total Step: 1026, Loss: 0.501953125
Epoch: 1, Total Step: 1036, Loss: 0.41845703125
[2023-10-12 11:46:56,881] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=3, lr=[0.0004818518037279798], mom=[(0.9, 0.95)]
[2023-10-12 11:46:56,881] [INFO] [timer.py:260:stop] epoch=1/micro_step=105/global_step=260, RunningAvgSamplesPerSec=2.2340846063825848, CurrSamplesPerSec=2.2670034045150445, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1046, Loss: 0.33251953125
Epoch: 1, Total Step: 1056, Loss: 0.333740234375
Epoch: 1, Total Step: 1066, Loss: 0.34130859375
Epoch: 1, Total Step: 1076, Loss: 0.30078125
[2023-10-12 11:49:14,172] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=3, lr=[0.0004804310889078861], mom=[(0.9, 0.95)]
[2023-10-12 11:49:14,173] [INFO] [timer.py:260:stop] epoch=1/micro_step=145/global_step=270, RunningAvgSamplesPerSec=2.2376006641317083, CurrSamplesPerSec=2.3219853016292147, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1086, Loss: 0.288818359375
Epoch: 1, Total Step: 1096, Loss: 0.232666015625
Epoch: 1, Total Step: 1106, Loss: 0.301513671875
Epoch: 1, Total Step: 1116, Loss: 0.3310546875
[2023-10-12 11:51:29,334] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=3, lr=[0.0004789590978644808], mom=[(0.9, 0.95)]
[2023-10-12 11:51:29,335] [INFO] [timer.py:260:stop] epoch=1/micro_step=185/global_step=280, RunningAvgSamplesPerSec=2.242075230118806, CurrSamplesPerSec=2.298043753288448, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1126, Loss: 0.30078125
Epoch: 1, Total Step: 1136, Loss: 0.369873046875
Epoch: 1, Total Step: 1146, Loss: 0.31103515625
Epoch: 1, Total Step: 1156, Loss: 0.51708984375
[2023-10-12 11:53:54,449] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=3, lr=[0.00047743615814966575], mom=[(0.9, 0.95)]
[2023-10-12 11:53:54,450] [INFO] [timer.py:260:stop] epoch=1/micro_step=225/global_step=290, RunningAvgSamplesPerSec=2.2408215044898627, CurrSamplesPerSec=1.5650364883438197, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1166, Loss: 0.208251953125
Epoch: 1, Total Step: 1176, Loss: 0.33447265625
Epoch: 1, Total Step: 1186, Loss: 0.39794921875
Epoch: 1, Total Step: 1196, Loss: 0.315185546875
[2023-10-12 11:56:44,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=3, lr=[0.0004758626086525956], mom=[(0.9, 0.95)]
[2023-10-12 11:56:44,696] [INFO] [timer.py:260:stop] epoch=1/micro_step=265/global_step=300, RunningAvgSamplesPerSec=2.2265201079292787, CurrSamplesPerSec=2.306182429575278, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1206, Loss: 0.314453125
Epoch: 1, Total Step: 1216, Loss: 0.495361328125
Epoch: 1, Total Step: 1226, Loss: 0.218994140625
Epoch: 1, Total Step: 1236, Loss: 0.305908203125
[2023-10-12 11:59:01,571] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=3, lr=[0.00047423879952426687], mom=[(0.9, 0.95)]
[2023-10-12 11:59:01,573] [INFO] [timer.py:260:stop] epoch=1/micro_step=305/global_step=310, RunningAvgSamplesPerSec=2.230008584400658, CurrSamplesPerSec=2.374226536886627, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1246, Loss: 0.300048828125
Epoch: 1, Total Step: 1256, Loss: 0.361328125
Epoch: 1, Total Step: 1266, Loss: 0.298095703125
Epoch: 1, Total Step: 1276, Loss: 0.271484375
[2023-10-12 12:01:17,351] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=3, lr=[0.0004725650920996012], mom=[(0.9, 0.95)]
[2023-10-12 12:01:17,351] [INFO] [timer.py:260:stop] epoch=1/micro_step=345/global_step=320, RunningAvgSamplesPerSec=2.2338351995815366, CurrSamplesPerSec=2.371310927640489, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1286, Loss: 0.2900390625
Epoch: 1, Total Step: 1296, Loss: 0.237060546875
Epoch: 1, Total Step: 1306, Loss: 0.27490234375
Epoch: 1, Total Step: 1316, Loss: 0.35009765625
[2023-10-12 12:03:33,029] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=3, lr=[0.0004708418588170403], mom=[(0.9, 0.95)]
[2023-10-12 12:03:33,029] [INFO] [timer.py:260:stop] epoch=1/micro_step=385/global_step=330, RunningAvgSamplesPerSec=2.237482320800243, CurrSamplesPerSec=2.3555116588919893, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1326, Loss: 0.340576171875
Epoch: 1, Total Step: 1336, Loss: 0.31689453125
Epoch: 1, Total Step: 1346, Loss: 0.362060546875
Epoch: 1, Total Step: 1356, Loss: 0.341796875
[2023-10-12 12:05:47,756] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=3, lr=[0.00046906948313566984], mom=[(0.9, 0.95)]
[2023-10-12 12:05:47,757] [INFO] [timer.py:260:stop] epoch=1/micro_step=425/global_step=340, RunningAvgSamplesPerSec=2.2413694899069148, CurrSamplesPerSec=2.3360723790201834, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1366, Loss: 0.3115234375
Epoch: 1, Total Step: 1376, Loss: 0.2489013671875
Epoch: 1, Total Step: 1386, Loss: 0.28271484375
Epoch: 1, Total Step: 1396, Loss: 0.3447265625
[2023-10-12 12:08:03,424] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=3, lr=[0.0004672483594498903], mom=[(0.9, 0.95)]
[2023-10-12 12:08:03,425] [INFO] [timer.py:260:stop] epoch=1/micro_step=465/global_step=350, RunningAvgSamplesPerSec=2.2446188764086905, CurrSamplesPerSec=2.402364272891436, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1406, Loss: 0.2412109375
Epoch: 1, Total Step: 1416, Loss: 0.2015380859375
Epoch: 1, Total Step: 1426, Loss: 0.33837890625
Epoch: 1, Total Step: 1436, Loss: 0.262451171875
[2023-10-12 12:10:20,134] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=3, lr=[0.00046537889300165614], mom=[(0.9, 0.95)]
[2023-10-12 12:10:20,134] [INFO] [timer.py:260:stop] epoch=1/micro_step=505/global_step=360, RunningAvgSamplesPerSec=2.2472334799133664, CurrSamplesPerSec=2.3053395811349335, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1446, Loss: 0.42431640625
Epoch: 1, Total Step: 1456, Loss: 0.183837890625
Epoch: 1, Total Step: 1466, Loss: 0.286376953125
Epoch: 1, Total Step: 1476, Loss: 0.25146484375
[2023-10-12 12:12:53,201] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=3, lr=[0.0004634614997902993], mom=[(0.9, 0.95)]
[2023-10-12 12:12:53,202] [INFO] [timer.py:260:stop] epoch=1/micro_step=545/global_step=370, RunningAvgSamplesPerSec=2.242705298362402, CurrSamplesPerSec=1.2373658128642717, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1486, Loss: 0.299560546875
Epoch: 1, Total Step: 1496, Loss: 0.264404296875
Epoch: 1, Total Step: 1506, Loss: 0.2340087890625
Epoch: 1, Total Step: 1516, Loss: 0.307373046875
[2023-10-12 12:15:46,428] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=3, lr=[0.00046149660647996026], mom=[(0.9, 0.95)]
[2023-10-12 12:15:46,429] [INFO] [timer.py:260:stop] epoch=1/micro_step=585/global_step=380, RunningAvgSamplesPerSec=2.2301123422906928, CurrSamplesPerSec=2.3291191678195036, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1526, Loss: 0.320556640625
Epoch: 1, Total Step: 1536, Loss: 0.37451171875
Epoch: 1, Total Step: 1546, Loss: 0.273681640625
Epoch: 1, Total Step: 1556, Loss: 0.3271484375
[2023-10-12 12:18:01,987] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=3, lr=[0.00045948465030464533], mom=[(0.9, 0.95)]
[2023-10-12 12:18:01,988] [INFO] [timer.py:260:stop] epoch=1/micro_step=625/global_step=390, RunningAvgSamplesPerSec=2.233334598552047, CurrSamplesPerSec=2.3780461413962475, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1566, Loss: 0.354248046875
Epoch: 1, Total Step: 1576, Loss: 0.2333984375
Epoch: 1, Total Step: 1586, Loss: 0.425048828125
Epoch: 1, Total Step: 1596, Loss: 0.32568359375
[2023-10-12 12:20:17,137] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=3, lr=[0.0004574260789709318], mom=[(0.9, 0.95)]
[2023-10-12 12:20:17,138] [INFO] [timer.py:260:stop] epoch=1/micro_step=665/global_step=400, RunningAvgSamplesPerSec=2.236559531544544, CurrSamplesPerSec=2.3818102991029053, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1606, Loss: 0.2176513671875
Epoch: 1, Total Step: 1616, Loss: 0.300537109375
[2023-10-12 12:21:39,042] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, but hysteresis is 2. Reducing hysteresis to 1
Epoch: 1, Total Step: 1626, Loss: 0.314697265625
[2023-10-12 12:21:52,646] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
Epoch: 1, Total Step: 1636, Loss: 0.3994140625
[2023-10-12 12:22:33,469] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=5, lr=[0.0004557459664734141], mom=[(0.9, 0.95)]
[2023-10-12 12:22:33,470] [INFO] [timer.py:260:stop] epoch=1/micro_step=705/global_step=410, RunningAvgSamplesPerSec=2.2391796501094348, CurrSamplesPerSec=2.347944500034086, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1646, Loss: 0.268310546875
Epoch: 1, Total Step: 1656, Loss: 0.357421875
Epoch: 1, Total Step: 1666, Loss: 0.3203125
Epoch: 1, Total Step: 1676, Loss: 0.306884765625
[2023-10-12 12:24:49,027] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=5, lr=[0.0004536046491205865], mom=[(0.9, 0.95)]
[2023-10-12 12:24:49,028] [INFO] [timer.py:260:stop] epoch=1/micro_step=745/global_step=420, RunningAvgSamplesPerSec=2.2419775085343585, CurrSamplesPerSec=2.353870360999717, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1686, Loss: 0.319580078125
Epoch: 1, Total Step: 1696, Loss: 0.36669921875
Epoch: 1, Total Step: 1706, Loss: 0.26708984375
Epoch: 1, Total Step: 1716, Loss: 0.31787109375
[2023-10-12 12:27:04,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=5, lr=[0.00045141802504501997], mom=[(0.9, 0.95)]
[2023-10-12 12:27:04,981] [INFO] [timer.py:260:stop] epoch=1/micro_step=785/global_step=430, RunningAvgSamplesPerSec=2.2445017684047976, CurrSamplesPerSec=2.329696599527905, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1726, Loss: 0.2489013671875
Epoch: 1, Total Step: 1736, Loss: 0.4267578125
Epoch: 1, Total Step: 1746, Loss: 0.20263671875
Epoch: 1, Total Step: 1756, Loss: 0.31640625
[2023-10-12 12:29:21,338] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=5, lr=[0.0004491865808209215], mom=[(0.9, 0.95)]
[2023-10-12 12:29:21,338] [INFO] [timer.py:260:stop] epoch=1/micro_step=825/global_step=440, RunningAvgSamplesPerSec=2.246771457666495, CurrSamplesPerSec=2.336210141743099, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1766, Loss: 0.28955078125
Epoch: 1, Total Step: 1776, Loss: 0.267333984375
Epoch: 1, Total Step: 1786, Loss: 0.231689453125
Epoch: 1, Total Step: 1796, Loss: 0.264404296875
[2023-10-12 12:31:38,474] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=5, lr=[0.0004469108129960135], mom=[(0.9, 0.95)]
[2023-10-12 12:31:38,475] [INFO] [timer.py:260:stop] epoch=1/micro_step=865/global_step=450, RunningAvgSamplesPerSec=2.2486693335536803, CurrSamplesPerSec=2.0774830719489206, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1806, Loss: 0.2705078125
Epoch: 1, Total Step: 1816, Loss: 0.259033203125
Epoch: 1, Total Step: 1826, Loss: 0.1815185546875
Epoch: 1, Total Step: 1836, Loss: 0.3046875
[2023-10-12 12:34:35,468] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=5, lr=[0.00044459122798104], mom=[(0.9, 0.95)]
[2023-10-12 12:34:35,469] [INFO] [timer.py:260:stop] epoch=1/micro_step=905/global_step=460, RunningAvgSamplesPerSec=2.2368024318259083, CurrSamplesPerSec=2.345682155561451, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 1, Total Step: 1846, Loss: 0.34228515625
Epoch: 1, Total Step: 1856, Loss: 0.1729736328125
Epoch: 1, Total Step: 1866, Loss: 0.27490234375
***** Evaluating perplexity, Epoch 2/9 *****
Invalidate trace cache @ step 0: expected module 0, but got module 6
ppl: 1.370121955871582
eval loss: 0.3148951530456543
Beginning of Epoch 3/9, Total Micro Batches 935
Epoch: 2, Total Step: 1871, Loss: 0.2357177734375
[2023-10-12 12:41:37,576] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=5, lr=[0.0004422283419370789], mom=[(0.9, 0.95)]
[2023-10-12 12:41:37,577] [INFO] [timer.py:260:stop] epoch=2/micro_step=10/global_step=470, RunningAvgSamplesPerSec=2.2378444910293793, CurrSamplesPerSec=2.347313569395018, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 1881, Loss: 0.3046875
Epoch: 2, Total Step: 1891, Loss: 0.34619140625
Epoch: 2, Total Step: 1901, Loss: 0.323486328125
Epoch: 2, Total Step: 1911, Loss: 0.490478515625
[2023-10-12 12:43:53,413] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=5, lr=[0.000439822680660684], mom=[(0.9, 0.95)]
[2023-10-12 12:43:53,414] [INFO] [timer.py:260:stop] epoch=2/micro_step=50/global_step=480, RunningAvgSamplesPerSec=2.2402204271670563, CurrSamplesPerSec=2.3265673948750583, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 1921, Loss: 0.281494140625
Epoch: 2, Total Step: 1931, Loss: 0.322021484375
Epoch: 2, Total Step: 1941, Loss: 0.257568359375
Epoch: 2, Total Step: 1951, Loss: 0.220947265625
[2023-10-12 12:46:09,222] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=5, lr=[0.00043737477946688343], mom=[(0.9, 0.95)]
[2023-10-12 12:46:09,223] [INFO] [timer.py:260:stop] epoch=2/micro_step=90/global_step=490, RunningAvgSamplesPerSec=2.242509532481639, CurrSamplesPerSec=2.3633772524768997, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 1961, Loss: 0.4716796875
Epoch: 2, Total Step: 1971, Loss: 0.400390625
Epoch: 2, Total Step: 1981, Loss: 0.30615234375
Epoch: 2, Total Step: 1991, Loss: 0.31201171875
[2023-10-12 12:48:25,441] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=5, lr=[0.0004348851830700593], mom=[(0.9, 0.95)]
[2023-10-12 12:48:25,442] [INFO] [timer.py:260:stop] epoch=2/micro_step=130/global_step=500, RunningAvgSamplesPerSec=2.2445853208777624, CurrSamplesPerSec=2.417747249734425, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2001, Loss: 0.32470703125
Epoch: 2, Total Step: 2011, Loss: 0.280029296875
Epoch: 2, Total Step: 2021, Loss: 0.263916015625
Epoch: 2, Total Step: 2031, Loss: 0.2236328125
[2023-10-12 12:50:15,242] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, but hysteresis is 2. Reducing hysteresis to 1
[2023-10-12 12:50:28,752] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
[2023-10-12 12:50:42,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=7, lr=[0.00043286385738079203], mom=[(0.9, 0.95)]
[2023-10-12 12:50:42,412] [INFO] [timer.py:260:stop] epoch=2/micro_step=170/global_step=510, RunningAvgSamplesPerSec=2.2463500636741234, CurrSamplesPerSec=2.344505212638449, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2041, Loss: 0.2880859375
Epoch: 2, Total Step: 2051, Loss: 0.3134765625
Epoch: 2, Total Step: 2061, Loss: 0.2861328125
Epoch: 2, Total Step: 2071, Loss: 0.351318359375
[2023-10-12 12:53:51,028] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=7, lr=[0.00043030061183595364], mom=[(0.9, 0.95)]
[2023-10-12 12:53:51,029] [INFO] [timer.py:260:stop] epoch=2/micro_step=210/global_step=520, RunningAvgSamplesPerSec=2.232413898731365, CurrSamplesPerSec=2.3035145066204423, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2081, Loss: 0.294189453125
Epoch: 2, Total Step: 2091, Loss: 0.492919921875
Epoch: 2, Total Step: 2101, Loss: 0.2015380859375
Epoch: 2, Total Step: 2111, Loss: 0.314208984375
[2023-10-12 12:56:06,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=7, lr=[0.0004276972452532833], mom=[(0.9, 0.95)]
[2023-10-12 12:56:06,237] [INFO] [timer.py:260:stop] epoch=2/micro_step=250/global_step=530, RunningAvgSamplesPerSec=2.2348421519801356, CurrSamplesPerSec=2.4079316191818414, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2121, Loss: 0.379638671875
Epoch: 2, Total Step: 2131, Loss: 0.292724609375
Epoch: 2, Total Step: 2141, Loss: 0.295654296875
Epoch: 2, Total Step: 2151, Loss: 0.468994140625
[2023-10-12 12:58:20,691] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=7, lr=[0.00042505433694179213], mom=[(0.9, 0.95)]
[2023-10-12 12:58:20,692] [INFO] [timer.py:260:stop] epoch=2/micro_step=290/global_step=540, RunningAvgSamplesPerSec=2.23740350449483, CurrSamplesPerSec=2.3519092685263128, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2161, Loss: 0.2081298828125
Epoch: 2, Total Step: 2171, Loss: 0.290283203125
Epoch: 2, Total Step: 2181, Loss: 0.28662109375
Epoch: 2, Total Step: 2191, Loss: 0.348876953125
[2023-10-12 13:00:37,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=7, lr=[0.00042237247500943653], mom=[(0.9, 0.95)]
[2023-10-12 13:00:37,123] [INFO] [timer.py:260:stop] epoch=2/micro_step=330/global_step=550, RunningAvgSamplesPerSec=2.2393147077843976, CurrSamplesPerSec=2.4089089717261056, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2201, Loss: 0.281005859375
Epoch: 2, Total Step: 2211, Loss: 0.255126953125
Epoch: 2, Total Step: 2221, Loss: 0.2724609375
Epoch: 2, Total Step: 2231, Loss: 0.224853515625
[2023-10-12 13:02:54,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=7, lr=[0.00041965225623225033], mom=[(0.9, 0.95)]
[2023-10-12 13:02:54,146] [INFO] [timer.py:260:stop] epoch=2/micro_step=370/global_step=560, RunningAvgSamplesPerSec=2.240991321739133, CurrSamplesPerSec=2.3227073126214317, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2241, Loss: 0.259765625
Epoch: 2, Total Step: 2251, Loss: 0.333251953125
Epoch: 2, Total Step: 2261, Loss: 0.32666015625
Epoch: 2, Total Step: 2271, Loss: 0.302978515625
[2023-10-12 13:05:11,409] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=7, lr=[0.00041689428592154876], mom=[(0.9, 0.95)]
[2023-10-12 13:05:11,410] [INFO] [timer.py:260:stop] epoch=2/micro_step=410/global_step=570, RunningAvgSamplesPerSec=2.242549800573823, CurrSamplesPerSec=2.320168880452718, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2281, Loss: 0.346923828125
Epoch: 2, Total Step: 2291, Loss: 0.31982421875
Epoch: 2, Total Step: 2301, Loss: 0.296142578125
Epoch: 2, Total Step: 2311, Loss: 0.2379150390625
[2023-10-12 13:07:29,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=7, lr=[0.0004140991777892324], mom=[(0.9, 0.95)]
[2023-10-12 13:07:29,271] [INFO] [timer.py:260:stop] epoch=2/micro_step=450/global_step=580, RunningAvgSamplesPerSec=2.2438910752537087, CurrSamplesPerSec=2.3148208629978315, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2321, Loss: 0.2705078125
Epoch: 2, Total Step: 2331, Loss: 0.32421875
Epoch: 2, Total Step: 2341, Loss: 0.2301025390625
Epoch: 2, Total Step: 2351, Loss: 0.18603515625
[2023-10-12 13:09:56,998] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=7, lr=[0.0004112675538112224], mom=[(0.9, 0.95)]
[2023-10-12 13:09:56,998] [INFO] [timer.py:260:stop] epoch=2/micro_step=490/global_step=590, RunningAvgSamplesPerSec=2.2425453321493145, CurrSamplesPerSec=1.7274334198411765, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2361, Loss: 0.32275390625
Epoch: 2, Total Step: 2371, Loss: 0.2490234375
Epoch: 2, Total Step: 2381, Loss: 0.396240234375
Epoch: 2, Total Step: 2391, Loss: 0.173828125
[2023-10-12 13:12:49,449] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=7, lr=[0.00040840004408905625], mom=[(0.9, 0.95)]
[2023-10-12 13:12:49,450] [INFO] [timer.py:260:stop] epoch=2/micro_step=530/global_step=600, RunningAvgSamplesPerSec=2.234777110408593, CurrSamplesPerSec=2.3314403738969656, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2401, Loss: 0.275146484375
Epoch: 2, Total Step: 2411, Loss: 0.240966796875
Epoch: 2, Total Step: 2421, Loss: 0.28662109375
Epoch: 2, Total Step: 2431, Loss: 0.249267578125
[2023-10-12 13:15:06,782] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, but hysteresis is 2. Reducing hysteresis to 1
[2023-10-12 13:15:06,783] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=8, lr=[0.00040578913024399564], mom=[(0.9, 0.95)]
[2023-10-12 13:15:06,784] [INFO] [timer.py:260:stop] epoch=2/micro_step=570/global_step=610, RunningAvgSamplesPerSec=2.236308664079737, CurrSamplesPerSec=2.3325952333925124, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2441, Loss: 0.220703125
[2023-10-12 13:15:20,154] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
Epoch: 2, Total Step: 2451, Loss: 0.288330078125
Epoch: 2, Total Step: 2461, Loss: 0.303466796875
Epoch: 2, Total Step: 2471, Loss: 0.36279296875
[2023-10-12 13:17:24,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=9, lr=[0.00040315013629830073], mom=[(0.9, 0.95)]
[2023-10-12 13:17:24,552] [INFO] [timer.py:260:stop] epoch=2/micro_step=610/global_step=620, RunningAvgSamplesPerSec=2.2376811815849096, CurrSamplesPerSec=2.309824587519732, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2481, Loss: 0.2548828125
Epoch: 2, Total Step: 2491, Loss: 0.303955078125
Epoch: 2, Total Step: 2501, Loss: 0.338134765625
Epoch: 2, Total Step: 2511, Loss: 0.2198486328125
[2023-10-12 13:19:42,595] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=9, lr=[0.00040018556605949475], mom=[(0.9, 0.95)]
[2023-10-12 13:19:42,596] [INFO] [timer.py:260:stop] epoch=2/micro_step=650/global_step=630, RunningAvgSamplesPerSec=2.2389433376306997, CurrSamplesPerSec=2.308877788816609, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2521, Loss: 0.40478515625
Epoch: 2, Total Step: 2531, Loss: 0.306884765625
Epoch: 2, Total Step: 2541, Loss: 0.2110595703125
Epoch: 2, Total Step: 2551, Loss: 0.283203125
[2023-10-12 13:22:00,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=9, lr=[0.00039718757607398725], mom=[(0.9, 0.95)]
[2023-10-12 13:22:00,353] [INFO] [timer.py:260:stop] epoch=2/micro_step=690/global_step=640, RunningAvgSamplesPerSec=2.2402318055525314, CurrSamplesPerSec=2.324590316062517, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2561, Loss: 0.312255859375
Epoch: 2, Total Step: 2571, Loss: 0.380859375
Epoch: 2, Total Step: 2581, Loss: 0.25
Epoch: 2, Total Step: 2591, Loss: 0.341064453125
[2023-10-12 13:24:17,184] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=9, lr=[0.0003941568334635833], mom=[(0.9, 0.95)]
[2023-10-12 13:24:17,184] [INFO] [timer.py:260:stop] epoch=2/micro_step=730/global_step=650, RunningAvgSamplesPerSec=2.2417089878625496, CurrSamplesPerSec=2.3747103749105176, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2601, Loss: 0.306884765625
Epoch: 2, Total Step: 2611, Loss: 0.289306640625
Epoch: 2, Total Step: 2621, Loss: 0.310791015625
Epoch: 2, Total Step: 2631, Loss: 0.3486328125
[2023-10-12 13:26:36,346] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=9, lr=[0.00039109401263830125], mom=[(0.9, 0.95)]
[2023-10-12 13:26:36,347] [INFO] [timer.py:260:stop] epoch=2/micro_step=770/global_step=660, RunningAvgSamplesPerSec=2.242586126892318, CurrSamplesPerSec=2.2090717192491254, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2641, Loss: 0.25341796875
Epoch: 2, Total Step: 2651, Loss: 0.3037109375
Epoch: 2, Total Step: 2661, Loss: 0.2357177734375
Epoch: 2, Total Step: 2671, Loss: 0.409912109375
[2023-10-12 13:29:21,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=9, lr=[0.00038799979514630084], mom=[(0.9, 0.95)]
[2023-10-12 13:29:21,681] [INFO] [timer.py:260:stop] epoch=2/micro_step=810/global_step=670, RunningAvgSamplesPerSec=2.2372925955165006, CurrSamplesPerSec=1.3063110852747002, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2681, Loss: 0.1915283203125
Epoch: 2, Total Step: 2691, Loss: 0.299072265625
Epoch: 2, Total Step: 2701, Loss: 0.2763671875
Epoch: 2, Total Step: 2711, Loss: 0.25439453125
[2023-10-12 13:32:30,204] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=9, lr=[0.00038487486952222316], mom=[(0.9, 0.95)]
[2023-10-12 13:32:30,204] [INFO] [timer.py:260:stop] epoch=2/micro_step=850/global_step=680, RunningAvgSamplesPerSec=2.226877655465432, CurrSamplesPerSec=2.2205674617068056, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2721, Loss: 0.2235107421875
Epoch: 2, Total Step: 2731, Loss: 0.253662109375
Epoch: 2, Total Step: 2741, Loss: 0.255859375
Epoch: 2, Total Step: 2751, Loss: 0.24755859375
[2023-10-12 13:34:53,435] [INFO] [logging.py:96:log_dist] [Rank 0] step=690, skipped=9, lr=[0.00038171993113397584], mom=[(0.9, 0.95)]
[2023-10-12 13:34:53,437] [INFO] [timer.py:260:stop] epoch=2/micro_step=890/global_step=690, RunningAvgSamplesPerSec=2.227002859101086, CurrSamplesPerSec=2.221322688810494, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2761, Loss: 0.1746826171875
Epoch: 2, Total Step: 2771, Loss: 0.289306640625
Epoch: 2, Total Step: 2781, Loss: 0.328125
Epoch: 2, Total Step: 2791, Loss: 0.1649169921875
[2023-10-12 13:37:16,609] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=9, lr=[0.0003785356820279975], mom=[(0.9, 0.95)]
[2023-10-12 13:37:16,609] [INFO] [timer.py:260:stop] epoch=2/micro_step=930/global_step=700, RunningAvgSamplesPerSec=2.2271382852263995, CurrSamplesPerSec=2.2264961172515063, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 2, Total Step: 2801, Loss: 0.261962890625
***** Evaluating perplexity, Epoch 3/9 *****
Invalidate trace cache @ step 0: expected module 0, but got module 6
ppl: 1.3676546812057495
eval loss: 0.3130929470062256
Beginning of Epoch 4/9, Total Micro Batches 935
Epoch: 3, Total Step: 2806, Loss: 0.218994140625
Epoch: 3, Total Step: 2816, Loss: 0.286376953125
Epoch: 3, Total Step: 2826, Loss: 0.330810546875
Epoch: 3, Total Step: 2836, Loss: 0.3037109375
[2023-10-12 13:44:57,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=710, skipped=9, lr=[0.0003753228307730364], mom=[(0.9, 0.95)]
[2023-10-12 13:44:57,278] [INFO] [timer.py:260:stop] epoch=3/micro_step=35/global_step=710, RunningAvgSamplesPerSec=2.2256258322924696, CurrSamplesPerSec=2.182986518260539, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 2846, Loss: 0.462890625
[2023-10-12 13:45:26,560] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, but hysteresis is 2. Reducing hysteresis to 1
[2023-10-12 13:45:41,314] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
Epoch: 3, Total Step: 2856, Loss: 0.273193359375
Epoch: 3, Total Step: 2866, Loss: 0.303955078125
Epoch: 3, Total Step: 2876, Loss: 0.2445068359375
[2023-10-12 13:47:25,664] [INFO] [logging.py:96:log_dist] [Rank 0] step=720, skipped=11, lr=[0.0003727324364472384], mom=[(0.9, 0.95)]
[2023-10-12 13:47:25,665] [INFO] [timer.py:260:stop] epoch=3/micro_step=75/global_step=720, RunningAvgSamplesPerSec=2.224654353701053, CurrSamplesPerSec=2.1225371798600485, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 2886, Loss: 0.213623046875
Epoch: 3, Total Step: 2896, Loss: 0.4453125
Epoch: 3, Total Step: 2906, Loss: 0.385498046875
Epoch: 3, Total Step: 2916, Loss: 0.286865234375
[2023-10-12 13:50:33,440] [INFO] [logging.py:96:log_dist] [Rank 0] step=730, skipped=11, lr=[0.00036946990713174774], mom=[(0.9, 0.95)]
[2023-10-12 13:50:33,440] [INFO] [timer.py:260:stop] epoch=3/micro_step=115/global_step=730, RunningAvgSamplesPerSec=2.2153811466001727, CurrSamplesPerSec=2.0960843725253815, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 2926, Loss: 0.292236328125
Epoch: 3, Total Step: 2936, Loss: 0.310791015625
Epoch: 3, Total Step: 2946, Loss: 0.265380859375
Epoch: 3, Total Step: 2956, Loss: 0.24658203125
[2023-10-12 13:53:57,248] [INFO] [logging.py:96:log_dist] [Rank 0] step=740, skipped=11, lr=[0.00036618079301094214], mom=[(0.9, 0.95)]
[2023-10-12 13:53:57,249] [INFO] [timer.py:260:stop] epoch=3/micro_step=155/global_step=740, RunningAvgSamplesPerSec=2.203137364627881, CurrSamplesPerSec=2.1906304812083177, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 2966, Loss: 0.2232666015625
Epoch: 3, Total Step: 2976, Loss: 0.276123046875
Epoch: 3, Total Step: 2986, Loss: 0.300048828125
Epoch: 3, Total Step: 2996, Loss: 0.274658203125
[2023-10-12 13:56:25,017] [INFO] [logging.py:96:log_dist] [Rank 0] step=750, skipped=11, lr=[0.00036286582598845046], mom=[(0.9, 0.95)]
[2023-10-12 13:56:25,018] [INFO] [timer.py:260:stop] epoch=3/micro_step=195/global_step=750, RunningAvgSamplesPerSec=2.2026425427031655, CurrSamplesPerSec=2.077886003312134, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 3006, Loss: 0.33642578125
Epoch: 3, Total Step: 3016, Loss: 0.281005859375
Epoch: 3, Total Step: 3026, Loss: 0.467041015625
Epoch: 3, Total Step: 3036, Loss: 0.193603515625
[2023-10-12 13:58:48,146] [INFO] [logging.py:96:log_dist] [Rank 0] step=760, skipped=11, lr=[0.00035952574372076734], mom=[(0.9, 0.95)]
[2023-10-12 13:58:48,147] [INFO] [timer.py:260:stop] epoch=3/micro_step=235/global_step=760, RunningAvgSamplesPerSec=2.2030895062288747, CurrSamplesPerSec=2.534634749871675, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 3046, Loss: 0.29833984375
Epoch: 3, Total Step: 3056, Loss: 0.36083984375
Epoch: 3, Total Step: 3066, Loss: 0.281005859375
Epoch: 3, Total Step: 3076, Loss: 0.2802734375
[2023-10-12 14:00:58,134] [INFO] [logging.py:96:log_dist] [Rank 0] step=770, skipped=11, lr=[0.000356161289453108], mom=[(0.9, 0.95)]
[2023-10-12 14:00:58,135] [INFO] [timer.py:260:stop] epoch=3/micro_step=275/global_step=770, RunningAvgSamplesPerSec=2.206122679854893, CurrSamplesPerSec=2.1379284078772605, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 3086, Loss: 0.44970703125
Epoch: 3, Total Step: 3096, Loss: 0.1986083984375
Epoch: 3, Total Step: 3106, Loss: 0.27685546875
Epoch: 3, Total Step: 3116, Loss: 0.273193359375
[2023-10-12 14:04:06,341] [INFO] [logging.py:96:log_dist] [Rank 0] step=780, skipped=11, lr=[0.0003527732118540185], mom=[(0.9, 0.95)]
[2023-10-12 14:04:06,342] [INFO] [timer.py:260:stop] epoch=3/micro_step=315/global_step=780, RunningAvgSamplesPerSec=2.197736513630086, CurrSamplesPerSec=2.5080867264803093, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 3126, Loss: 0.3369140625
Epoch: 3, Total Step: 3136, Loss: 0.269775390625
Epoch: 3, Total Step: 3146, Loss: 0.240966796875
Epoch: 3, Total Step: 3156, Loss: 0.257568359375
[2023-10-12 14:06:15,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=790, skipped=11, lr=[0.0003493622648487805], mom=[(0.9, 0.95)]
[2023-10-12 14:06:15,175] [INFO] [timer.py:260:stop] epoch=3/micro_step=355/global_step=790, RunningAvgSamplesPerSec=2.2009730235833804, CurrSamplesPerSec=2.4330154069293495, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 3166, Loss: 0.215087890625
Epoch: 3, Total Step: 3176, Loss: 0.2493896484375
Epoch: 3, Total Step: 3186, Loss: 0.317626953125
Epoch: 3, Total Step: 3196, Loss: 0.3134765625
[2023-10-12 14:08:17,406] [INFO] [logging.py:96:log_dist] [Rank 0] step=800, skipped=11, lr=[0.0003459292074516449], mom=[(0.9, 0.95)]
[2023-10-12 14:08:17,406] [INFO] [timer.py:260:stop] epoch=3/micro_step=395/global_step=800, RunningAvgSamplesPerSec=2.2053922289685115, CurrSamplesPerSec=2.688360378265238, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 3206, Loss: 0.289306640625
Epoch: 3, Total Step: 3216, Loss: 0.330322265625
Epoch: 3, Total Step: 3226, Loss: 0.304443359375
Epoch: 3, Total Step: 3236, Loss: 0.283447265625
[2023-10-12 14:10:31,583] [INFO] [logging.py:96:log_dist] [Rank 0] step=810, skipped=11, lr=[0.000342474803596934], mom=[(0.9, 0.95)]
[2023-10-12 14:10:31,584] [INFO] [timer.py:260:stop] epoch=3/micro_step=435/global_step=810, RunningAvgSamplesPerSec=2.207462170945851, CurrSamplesPerSec=2.3083056278654595, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 3246, Loss: 0.2264404296875
Epoch: 3, Total Step: 3256, Loss: 0.258544921875
[2023-10-12 14:11:27,754] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, but hysteresis is 2. Reducing hysteresis to 1
[2023-10-12 14:11:41,583] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
Epoch: 3, Total Step: 3266, Loss: 0.30615234375
Epoch: 3, Total Step: 3276, Loss: 0.22265625
[2023-10-12 14:12:50,036] [INFO] [logging.py:96:log_dist] [Rank 0] step=820, skipped=13, lr=[0.00033969642746438833], mom=[(0.9, 0.95)]
[2023-10-12 14:12:50,036] [INFO] [timer.py:260:stop] epoch=3/micro_step=475/global_step=820, RunningAvgSamplesPerSec=2.2086868241868696, CurrSamplesPerSec=2.3495486267730645, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 3286, Loss: 0.17578125
Epoch: 3, Total Step: 3296, Loss: 0.310302734375
Epoch: 3, Total Step: 3306, Loss: 0.236328125
Epoch: 3, Total Step: 3316, Loss: 0.375732421875
[2023-10-12 14:15:03,629] [INFO] [logging.py:96:log_dist] [Rank 0] step=830, skipped=13, lr=[0.00033620554015361206], mom=[(0.9, 0.95)]
[2023-10-12 14:15:03,629] [INFO] [timer.py:260:stop] epoch=3/micro_step=515/global_step=830, RunningAvgSamplesPerSec=2.2107787013349434, CurrSamplesPerSec=2.388465909641962, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 3326, Loss: 0.1666259765625
Epoch: 3, Total Step: 3336, Loss: 0.265869140625
Epoch: 3, Total Step: 3346, Loss: 0.2332763671875
Epoch: 3, Total Step: 3356, Loss: 0.275146484375
[2023-10-12 14:17:18,410] [INFO] [logging.py:96:log_dist] [Rank 0] step=840, skipped=13, lr=[0.0003326954701251366], mom=[(0.9, 0.95)]
[2023-10-12 14:17:18,410] [INFO] [timer.py:260:stop] epoch=3/micro_step=555/global_step=840, RunningAvgSamplesPerSec=2.212609192656967, CurrSamplesPerSec=2.364492744036051, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 3366, Loss: 0.23486328125
Epoch: 3, Total Step: 3376, Loss: 0.2237548828125
Epoch: 3, Total Step: 3386, Loss: 0.27197265625
Epoch: 3, Total Step: 3396, Loss: 0.288330078125
[2023-10-12 14:19:26,588] [INFO] [logging.py:96:log_dist] [Rank 0] step=850, skipped=13, lr=[0.00032916699845036816], mom=[(0.9, 0.95)]
[2023-10-12 14:19:26,588] [INFO] [timer.py:260:stop] epoch=3/micro_step=595/global_step=850, RunningAvgSamplesPerSec=2.215590716502522, CurrSamplesPerSec=2.774437206779482, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 3406, Loss: 0.3486328125
Epoch: 3, Total Step: 3416, Loss: 0.2396240234375
Epoch: 3, Total Step: 3426, Loss: 0.284423828125
Epoch: 3, Total Step: 3436, Loss: 0.323974609375
[2023-10-12 14:21:26,678] [INFO] [logging.py:96:log_dist] [Rank 0] step=860, skipped=13, lr=[0.00032562091029550276], mom=[(0.9, 0.95)]
[2023-10-12 14:21:26,678] [INFO] [timer.py:260:stop] epoch=3/micro_step=635/global_step=860, RunningAvgSamplesPerSec=2.2199598703953525, CurrSamplesPerSec=2.388681678645545, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 3446, Loss: 0.2091064453125
Epoch: 3, Total Step: 3456, Loss: 0.39208984375
Epoch: 3, Total Step: 3466, Loss: 0.292724609375
Epoch: 3, Total Step: 3476, Loss: 0.2037353515625
[2023-10-12 14:23:40,129] [INFO] [logging.py:96:log_dist] [Rank 0] step=870, skipped=13, lr=[0.00032205799474680896], mom=[(0.9, 0.95)]
[2023-10-12 14:23:40,132] [INFO] [timer.py:260:stop] epoch=3/micro_step=675/global_step=870, RunningAvgSamplesPerSec=2.221871646347713, CurrSamplesPerSec=2.3640870949219583, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 3486, Loss: 0.27001953125
Epoch: 3, Total Step: 3496, Loss: 0.296875
Epoch: 3, Total Step: 3506, Loss: 0.364990234375
Epoch: 3, Total Step: 3516, Loss: 0.234130859375
[2023-10-12 14:25:55,345] [INFO] [logging.py:96:log_dist] [Rank 0] step=880, skipped=13, lr=[0.0003184790446350381], mom=[(0.9, 0.95)]
[2023-10-12 14:25:55,346] [INFO] [timer.py:260:stop] epoch=3/micro_step=715/global_step=880, RunningAvgSamplesPerSec=2.2234335039077, CurrSamplesPerSec=2.3850327580607544, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 3526, Loss: 0.32568359375
Epoch: 3, Total Step: 3536, Loss: 0.297119140625
Epoch: 3, Total Step: 3546, Loss: 0.275390625
Epoch: 3, Total Step: 3556, Loss: 0.298583984375
[2023-10-12 14:28:11,365] [INFO] [logging.py:96:log_dist] [Rank 0] step=890, skipped=13, lr=[0.00031488485635900083], mom=[(0.9, 0.95)]
[2023-10-12 14:28:11,366] [INFO] [timer.py:260:stop] epoch=3/micro_step=755/global_step=890, RunningAvgSamplesPerSec=2.2248239404657144, CurrSamplesPerSec=2.334142279840676, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 3566, Loss: 0.3330078125
Epoch: 3, Total Step: 3576, Loss: 0.2412109375
Epoch: 3, Total Step: 3586, Loss: 0.2890625
Epoch: 3, Total Step: 3596, Loss: 0.222900390625
[2023-10-12 14:30:28,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=900, skipped=13, lr=[0.00031127622970835014], mom=[(0.9, 0.95)]
[2023-10-12 14:30:28,014] [INFO] [timer.py:260:stop] epoch=3/micro_step=795/global_step=900, RunningAvgSamplesPerSec=2.2260749416127803, CurrSamplesPerSec=2.3380822011637967, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 3606, Loss: 0.39599609375
Epoch: 3, Total Step: 3616, Loss: 0.1824951171875
Epoch: 3, Total Step: 3626, Loss: 0.285400390625
Epoch: 3, Total Step: 3636, Loss: 0.262451171875
[2023-10-12 14:32:44,355] [INFO] [logging.py:96:log_dist] [Rank 0] step=910, skipped=13, lr=[0.00030765396768561005], mom=[(0.9, 0.95)]
[2023-10-12 14:32:44,356] [INFO] [timer.py:260:stop] epoch=3/micro_step=835/global_step=910, RunningAvgSamplesPerSec=2.227350305222547, CurrSamplesPerSec=2.35983318007286, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 3646, Loss: 0.243896484375
Epoch: 3, Total Step: 3656, Loss: 0.2183837890625
[2023-10-12 14:34:05,640] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, but hysteresis is 2. Reducing hysteresis to 1
Epoch: 3, Total Step: 3666, Loss: 0.243896484375
[2023-10-12 14:34:19,183] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
Epoch: 3, Total Step: 3676, Loss: 0.2467041015625
[2023-10-12 14:34:59,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=15, lr=[0.00030474688215880987], mom=[(0.9, 0.95)]
[2023-10-12 14:34:59,729] [INFO] [timer.py:260:stop] epoch=3/micro_step=875/global_step=920, RunningAvgSamplesPerSec=2.228764308880845, CurrSamplesPerSec=2.3779818890811595, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 3686, Loss: 0.238525390625
Epoch: 3, Total Step: 3696, Loss: 0.169921875
Epoch: 3, Total Step: 3706, Loss: 0.277587890625
Epoch: 3, Total Step: 3716, Loss: 0.31103515625
[2023-10-12 14:37:14,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=930, skipped=15, lr=[0.0003011021096032973], mom=[(0.9, 0.95)]
[2023-10-12 14:37:14,884] [INFO] [timer.py:260:stop] epoch=3/micro_step=915/global_step=930, RunningAvgSamplesPerSec=2.2301867131075923, CurrSamplesPerSec=2.4276511798698404, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 3, Total Step: 3726, Loss: 0.1563720703125
Epoch: 3, Total Step: 3736, Loss: 0.25146484375
***** Evaluating perplexity, Epoch 4/9 *****
Invalidate trace cache @ step 0: expected module 0, but got module 6
ppl: 1.3701441287994385
eval loss: 0.314911425113678
Beginning of Epoch 5/9, Total Micro Batches 935
Epoch: 4, Total Step: 3741, Loss: 0.205078125
Epoch: 4, Total Step: 3751, Loss: 0.274169921875
[2023-10-12 14:47:37,157] [INFO] [logging.py:96:log_dist] [Rank 0] step=940, skipped=15, lr=[0.00029744596565169334], mom=[(0.9, 0.95)]
[2023-10-12 14:47:37,157] [INFO] [timer.py:260:stop] epoch=4/micro_step=20/global_step=940, RunningAvgSamplesPerSec=2.225558034004012, CurrSamplesPerSec=2.449778203891216, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 3761, Loss: 0.31298828125
Epoch: 4, Total Step: 3771, Loss: 0.28662109375
Epoch: 4, Total Step: 3781, Loss: 0.462158203125
Epoch: 4, Total Step: 3791, Loss: 0.26220703125
[2023-10-12 14:49:48,657] [INFO] [logging.py:96:log_dist] [Rank 0] step=950, skipped=15, lr=[0.00029377926388021573], mom=[(0.9, 0.95)]
[2023-10-12 14:49:48,658] [INFO] [timer.py:260:stop] epoch=4/micro_step=60/global_step=950, RunningAvgSamplesPerSec=2.2275768205151047, CurrSamplesPerSec=2.3826835233047596, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 3801, Loss: 0.287353515625
Epoch: 4, Total Step: 3811, Loss: 0.230224609375
Epoch: 4, Total Step: 3821, Loss: 0.2041015625
Epoch: 4, Total Step: 3831, Loss: 0.42431640625
[2023-10-12 14:52:04,322] [INFO] [logging.py:96:log_dist] [Rank 0] step=960, skipped=15, lr=[0.0002901028202144401], mom=[(0.9, 0.95)]
[2023-10-12 14:52:04,322] [INFO] [timer.py:260:stop] epoch=4/micro_step=100/global_step=960, RunningAvgSamplesPerSec=2.228881892243655, CurrSamplesPerSec=2.333787069188864, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 3841, Loss: 0.3720703125
Epoch: 4, Total Step: 3851, Loss: 0.2744140625
Epoch: 4, Total Step: 3861, Loss: 0.2763671875
Epoch: 4, Total Step: 3871, Loss: 0.300537109375
[2023-10-12 14:54:19,056] [INFO] [logging.py:96:log_dist] [Rank 0] step=970, skipped=15, lr=[0.0002864174527477376], mom=[(0.9, 0.95)]
[2023-10-12 14:54:19,056] [INFO] [timer.py:260:stop] epoch=4/micro_step=140/global_step=970, RunningAvgSamplesPerSec=2.230311857099842, CurrSamplesPerSec=2.3788137720796585, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 3881, Loss: 0.2529296875
Epoch: 4, Total Step: 3891, Loss: 0.2352294921875
Epoch: 4, Total Step: 3901, Loss: 0.2174072265625
Epoch: 4, Total Step: 3911, Loss: 0.264892578125
[2023-10-12 14:56:34,826] [INFO] [logging.py:96:log_dist] [Rank 0] step=980, skipped=15, lr=[0.000282723981559231], mom=[(0.9, 0.95)]
[2023-10-12 14:56:34,826] [INFO] [timer.py:260:stop] epoch=4/micro_step=180/global_step=980, RunningAvgSamplesPerSec=2.23154922376133, CurrSamplesPerSec=2.34242150202721, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 3921, Loss: 0.287353515625
Epoch: 4, Total Step: 3931, Loss: 0.265625
Epoch: 4, Total Step: 3941, Loss: 0.324951171875
Epoch: 4, Total Step: 3951, Loss: 0.267333984375
[2023-10-12 14:58:50,678] [INFO] [logging.py:96:log_dist] [Rank 0] step=990, skipped=15, lr=[0.0002790232285313076], mom=[(0.9, 0.95)]
[2023-10-12 14:58:50,679] [INFO] [timer.py:260:stop] epoch=4/micro_step=220/global_step=990, RunningAvgSamplesPerSec=2.2327496987508653, CurrSamplesPerSec=2.358841922929836, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 3961, Loss: 0.4443359375
Epoch: 4, Total Step: 3971, Loss: 0.1885986328125
Epoch: 4, Total Step: 3981, Loss: 0.284912109375
Epoch: 4, Total Step: 3991, Loss: 0.3388671875
[2023-10-12 15:01:05,233] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=15, lr=[0.00027531601716673153], mom=[(0.9, 0.95)]
[2023-10-12 15:01:05,234] [INFO] [timer.py:260:stop] epoch=4/micro_step=260/global_step=1000, RunningAvgSamplesPerSec=2.2341308271711875, CurrSamplesPerSec=2.3852295103415186, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 4001, Loss: 0.270751953125
Epoch: 4, Total Step: 4011, Loss: 0.26611328125
Epoch: 4, Total Step: 4021, Loss: 0.433837890625
Epoch: 4, Total Step: 4031, Loss: 0.191650390625
[2023-10-12 15:03:19,606] [INFO] [logging.py:96:log_dist] [Rank 0] step=1010, skipped=15, lr=[0.00027160317240539566], mom=[(0.9, 0.95)]
[2023-10-12 15:03:19,607] [INFO] [timer.py:260:stop] epoch=4/micro_step=300/global_step=1010, RunningAvgSamplesPerSec=2.235513732342848, CurrSamplesPerSec=2.4226177923982846, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 4041, Loss: 0.265380859375
Epoch: 4, Total Step: 4051, Loss: 0.260986328125
Epoch: 4, Total Step: 4061, Loss: 0.324951171875
Epoch: 4, Total Step: 4071, Loss: 0.25927734375
[2023-10-12 15:05:06,128] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, but hysteresis is 2. Reducing hysteresis to 1
[2023-10-12 15:05:19,715] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
[2023-10-12 15:05:33,412] [INFO] [logging.py:96:log_dist] [Rank 0] step=1020, skipped=17, lr=[0.0002686293957152344], mom=[(0.9, 0.95)]
[2023-10-12 15:05:33,413] [INFO] [timer.py:260:stop] epoch=4/micro_step=340/global_step=1020, RunningAvgSamplesPerSec=2.236957649456468, CurrSamplesPerSec=2.3372922333278474, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 4081, Loss: 0.2301025390625
Epoch: 4, Total Step: 4091, Loss: 0.2449951171875
Epoch: 4, Total Step: 4101, Loss: 0.2060546875
Epoch: 4, Total Step: 4111, Loss: 0.24365234375
[2023-10-12 15:08:43,562] [INFO] [logging.py:96:log_dist] [Rank 0] step=1030, skipped=17, lr=[0.0002649084935722644], mom=[(0.9, 0.95)]
[2023-10-12 15:08:43,563] [INFO] [timer.py:260:stop] epoch=4/micro_step=380/global_step=1030, RunningAvgSamplesPerSec=2.2298309147722635, CurrSamplesPerSec=1.066240049032302, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 4121, Loss: 0.306884765625
Epoch: 4, Total Step: 4131, Loss: 0.302734375
Epoch: 4, Total Step: 4141, Loss: 0.279052734375
Epoch: 4, Total Step: 4151, Loss: 0.31689453125
[2023-10-12 15:13:05,606] [INFO] [logging.py:96:log_dist] [Rank 0] step=1040, skipped=17, lr=[0.0002611842739461835], mom=[(0.9, 0.95)]
[2023-10-12 15:13:05,607] [INFO] [timer.py:260:stop] epoch=4/micro_step=420/global_step=1040, RunningAvgSamplesPerSec=2.212255444827365, CurrSamplesPerSec=1.0983485752577173, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 4161, Loss: 0.290771484375
Epoch: 4, Total Step: 4171, Loss: 0.271240234375
Epoch: 4, Total Step: 4181, Loss: 0.21630859375
Epoch: 4, Total Step: 4191, Loss: 0.2578125
[2023-10-12 15:15:37,051] [INFO] [logging.py:96:log_dist] [Rank 0] step=1050, skipped=17, lr=[0.00025745756556161463], mom=[(0.9, 0.95)]
[2023-10-12 15:15:37,053] [INFO] [timer.py:260:stop] epoch=4/micro_step=460/global_step=1050, RunningAvgSamplesPerSec=2.211277430860552, CurrSamplesPerSec=2.4043977828361394, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 4201, Loss: 0.289794921875
Epoch: 4, Total Step: 4211, Loss: 0.2178955078125
Epoch: 4, Total Step: 4221, Loss: 0.169189453125
Epoch: 4, Total Step: 4231, Loss: 0.29443359375
[2023-10-12 15:17:49,830] [INFO] [logging.py:96:log_dist] [Rank 0] step=1060, skipped=17, lr=[0.0002537291976969864], mom=[(0.9, 0.95)]
[2023-10-12 15:17:49,831] [INFO] [timer.py:260:stop] epoch=4/micro_step=500/global_step=1060, RunningAvgSamplesPerSec=2.21301338613817, CurrSamplesPerSec=2.4462441506354193, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 4241, Loss: 0.2274169921875
Epoch: 4, Total Step: 4251, Loss: 0.353515625
Epoch: 4, Total Step: 4261, Loss: 0.1591796875
Epoch: 4, Total Step: 4271, Loss: 0.256103515625
[2023-10-12 15:20:00,006] [INFO] [logging.py:96:log_dist] [Rank 0] step=1070, skipped=17, lr=[0.00025], mom=[(0.9, 0.95)]
[2023-10-12 15:20:00,007] [INFO] [timer.py:260:stop] epoch=4/micro_step=540/global_step=1070, RunningAvgSamplesPerSec=2.215092938364252, CurrSamplesPerSec=2.4897913118131485, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 4281, Loss: 0.225830078125
Epoch: 4, Total Step: 4291, Loss: 0.26708984375
Epoch: 4, Total Step: 4301, Loss: 0.2237548828125
Epoch: 4, Total Step: 4311, Loss: 0.208251953125
[2023-10-12 15:22:08,720] [INFO] [logging.py:96:log_dist] [Rank 0] step=1080, skipped=17, lr=[0.00024627080230301367], mom=[(0.9, 0.95)]
[2023-10-12 15:22:08,721] [INFO] [timer.py:260:stop] epoch=4/micro_step=580/global_step=1080, RunningAvgSamplesPerSec=2.2173461358675595, CurrSamplesPerSec=2.486432126061983, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 4321, Loss: 0.2626953125
Epoch: 4, Total Step: 4331, Loss: 0.2744140625
Epoch: 4, Total Step: 4341, Loss: 0.339599609375
Epoch: 4, Total Step: 4351, Loss: 0.22900390625
[2023-10-12 15:24:17,367] [INFO] [logging.py:96:log_dist] [Rank 0] step=1090, skipped=17, lr=[0.00024254243443838538], mom=[(0.9, 0.95)]
[2023-10-12 15:24:17,367] [INFO] [timer.py:260:stop] epoch=4/micro_step=620/global_step=1090, RunningAvgSamplesPerSec=2.2195715755280863, CurrSamplesPerSec=2.476032980461607, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 4361, Loss: 0.265625
Epoch: 4, Total Step: 4371, Loss: 0.314208984375
Epoch: 4, Total Step: 4381, Loss: 0.201171875
Epoch: 4, Total Step: 4391, Loss: 0.380615234375
[2023-10-12 15:26:27,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=1100, skipped=17, lr=[0.00023881572605381648], mom=[(0.9, 0.95)]
[2023-10-12 15:26:27,597] [INFO] [timer.py:260:stop] epoch=4/micro_step=660/global_step=1100, RunningAvgSamplesPerSec=2.2215392709116517, CurrSamplesPerSec=2.497604570213632, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 4401, Loss: 0.2783203125
Epoch: 4, Total Step: 4411, Loss: 0.19677734375
Epoch: 4, Total Step: 4421, Loss: 0.259033203125
Epoch: 4, Total Step: 4431, Loss: 0.285888671875
[2023-10-12 15:28:37,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=1110, skipped=17, lr=[0.00023509150642773567], mom=[(0.9, 0.95)]
[2023-10-12 15:28:37,668] [INFO] [timer.py:260:stop] epoch=4/micro_step=700/global_step=1110, RunningAvgSamplesPerSec=2.223496388359715, CurrSamplesPerSec=2.4975747788537994, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 4441, Loss: 0.351318359375
Epoch: 4, Total Step: 4451, Loss: 0.22314453125
Epoch: 4, Total Step: 4461, Loss: 0.314208984375
Epoch: 4, Total Step: 4471, Loss: 0.28955078125
[2023-10-12 15:30:45,385] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, but hysteresis is 2. Reducing hysteresis to 1
[2023-10-12 15:30:45,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=1120, skipped=18, lr=[0.0002317425216080865], mom=[(0.9, 0.95)]
[2023-10-12 15:30:45,386] [INFO] [timer.py:260:stop] epoch=4/micro_step=740/global_step=1120, RunningAvgSamplesPerSec=2.225747398742468, CurrSamplesPerSec=2.500019706941023, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 4481, Loss: 0.263427734375
[2023-10-12 15:30:58,189] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
Epoch: 4, Total Step: 4491, Loss: 0.287109375
Epoch: 4, Total Step: 4501, Loss: 0.321044921875
Epoch: 4, Total Step: 4511, Loss: 0.230712890625
[2023-10-12 15:32:45,564] [INFO] [logging.py:96:log_dist] [Rank 0] step=1130, skipped=19, lr=[0.00022839682759460446], mom=[(0.9, 0.95)]
[2023-10-12 15:32:45,564] [INFO] [timer.py:260:stop] epoch=4/micro_step=780/global_step=1130, RunningAvgSamplesPerSec=2.2289991655191215, CurrSamplesPerSec=2.6215397413948485, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 4521, Loss: 0.27685546875
Epoch: 4, Total Step: 4531, Loss: 0.2120361328125
Epoch: 4, Total Step: 4541, Loss: 0.384033203125
Epoch: 4, Total Step: 4551, Loss: 0.1749267578125
[2023-10-12 15:34:49,110] [INFO] [logging.py:96:log_dist] [Rank 0] step=1140, skipped=19, lr=[0.00022468398283326856], mom=[(0.9, 0.95)]
[2023-10-12 15:34:49,111] [INFO] [timer.py:260:stop] epoch=4/micro_step=820/global_step=1140, RunningAvgSamplesPerSec=2.2317438112838985, CurrSamplesPerSec=2.5946340852538277, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 4561, Loss: 0.273193359375
Epoch: 4, Total Step: 4571, Loss: 0.2493896484375
Epoch: 4, Total Step: 4581, Loss: 0.2320556640625
Epoch: 4, Total Step: 4591, Loss: 0.21044921875
[2023-10-12 15:36:53,486] [INFO] [logging.py:96:log_dist] [Rank 0] step=1150, skipped=19, lr=[0.0002209767714686924], mom=[(0.9, 0.95)]
[2023-10-12 15:36:53,487] [INFO] [timer.py:260:stop] epoch=4/micro_step=860/global_step=1150, RunningAvgSamplesPerSec=2.234333734718614, CurrSamplesPerSec=2.569389144095417, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 4601, Loss: 0.2457275390625
Epoch: 4, Total Step: 4611, Loss: 0.23779296875
Epoch: 4, Total Step: 4621, Loss: 0.2315673828125
Epoch: 4, Total Step: 4631, Loss: 0.166748046875
[2023-10-12 15:38:56,900] [INFO] [logging.py:96:log_dist] [Rank 0] step=1160, skipped=19, lr=[0.00021727601844076896], mom=[(0.9, 0.95)]
[2023-10-12 15:38:56,901] [INFO] [timer.py:260:stop] epoch=4/micro_step=900/global_step=1160, RunningAvgSamplesPerSec=2.237016036147199, CurrSamplesPerSec=2.598887795834951, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 4, Total Step: 4641, Loss: 0.265380859375
Epoch: 4, Total Step: 4651, Loss: 0.29931640625
Epoch: 4, Total Step: 4661, Loss: 0.1513671875
Epoch: 4, Total Step: 4671, Loss: 0.2437744140625
***** Evaluating perplexity, Epoch 5/9 *****
Invalidate trace cache @ step 0: expected module 0, but got module 6
ppl: 1.375758409500122
eval loss: 0.319000780582428
Beginning of Epoch 6/9, Total Micro Batches 935
Epoch: 5, Total Step: 4676, Loss: 0.193603515625
[2023-10-12 15:45:22,370] [INFO] [logging.py:96:log_dist] [Rank 0] step=1170, skipped=19, lr=[0.0002135825472522624], mom=[(0.9, 0.95)]
[2023-10-12 15:45:22,371] [INFO] [timer.py:260:stop] epoch=5/micro_step=5/global_step=1170, RunningAvgSamplesPerSec=2.239297100280229, CurrSamplesPerSec=2.305483326428676, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 4686, Loss: 0.26123046875
Epoch: 5, Total Step: 4696, Loss: 0.297119140625
Epoch: 5, Total Step: 4706, Loss: 0.271728515625
Epoch: 5, Total Step: 4716, Loss: 0.434326171875
[2023-10-12 15:47:24,713] [INFO] [logging.py:96:log_dist] [Rank 0] step=1180, skipped=19, lr=[0.0002098971797855599], mom=[(0.9, 0.95)]
[2023-10-12 15:47:24,714] [INFO] [timer.py:260:stop] epoch=5/micro_step=45/global_step=1180, RunningAvgSamplesPerSec=2.242046191726503, CurrSamplesPerSec=2.603490559771972, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 4726, Loss: 0.25439453125
Epoch: 5, Total Step: 4736, Loss: 0.275390625
Epoch: 5, Total Step: 4746, Loss: 0.2186279296875
Epoch: 5, Total Step: 4756, Loss: 0.1962890625
[2023-10-12 15:49:27,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=1190, skipped=19, lr=[0.00020622073611978423], mom=[(0.9, 0.95)]
[2023-10-12 15:49:27,342] [INFO] [timer.py:260:stop] epoch=5/micro_step=85/global_step=1190, RunningAvgSamplesPerSec=2.244717675415486, CurrSamplesPerSec=2.5924785211488848, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 4766, Loss: 0.401123046875
Epoch: 5, Total Step: 4776, Loss: 0.35791015625
Epoch: 5, Total Step: 4786, Loss: 0.263916015625
Epoch: 5, Total Step: 4796, Loss: 0.262939453125
[2023-10-12 15:52:41,538] [INFO] [logging.py:96:log_dist] [Rank 0] step=1200, skipped=19, lr=[0.00020255403434830661], mom=[(0.9, 0.95)]
[2023-10-12 15:52:41,538] [INFO] [timer.py:260:stop] epoch=5/micro_step=125/global_step=1200, RunningAvgSamplesPerSec=2.2379626723320136, CurrSamplesPerSec=1.5380523362392877, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 4806, Loss: 0.292236328125
Epoch: 5, Total Step: 4816, Loss: 0.2427978515625
Epoch: 5, Total Step: 4826, Loss: 0.2254638671875
Epoch: 5, Total Step: 4836, Loss: 0.2132568359375
[2023-10-12 15:56:59,166] [INFO] [logging.py:96:log_dist] [Rank 0] step=1210, skipped=19, lr=[0.00019889789039670276], mom=[(0.9, 0.95)]
[2023-10-12 15:56:59,167] [INFO] [timer.py:260:stop] epoch=5/micro_step=165/global_step=1210, RunningAvgSamplesPerSec=2.223222475587252, CurrSamplesPerSec=2.559090899124218, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 4846, Loss: 0.255126953125
Epoch: 5, Total Step: 4856, Loss: 0.277587890625
Epoch: 5, Total Step: 4866, Loss: 0.25830078125
Epoch: 5, Total Step: 4876, Loss: 0.314697265625
[2023-10-12 15:59:08,532] [INFO] [logging.py:96:log_dist] [Rank 0] step=1220, skipped=19, lr=[0.0001952531178411902], mom=[(0.9, 0.95)]
[2023-10-12 15:59:08,534] [INFO] [timer.py:260:stop] epoch=5/micro_step=205/global_step=1220, RunningAvgSamplesPerSec=2.225081154336565, CurrSamplesPerSec=2.552123381871338, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 4886, Loss: 0.25537109375
[2023-10-12 15:59:33,603] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, but hysteresis is 2. Reducing hysteresis to 1
[2023-10-12 15:59:46,160] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
Epoch: 5, Total Step: 4896, Loss: 0.431640625
Epoch: 5, Total Step: 4906, Loss: 0.18408203125
Epoch: 5, Total Step: 4916, Loss: 0.27294921875
[2023-10-12 16:01:14,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=1230, skipped=21, lr=[0.00019234603231438997], mom=[(0.9, 0.95)]
[2023-10-12 16:01:14,198] [INFO] [timer.py:260:stop] epoch=5/micro_step=245/global_step=1230, RunningAvgSamplesPerSec=2.227381291558665, CurrSamplesPerSec=2.541849624418411, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 4926, Loss: 0.32080078125
Epoch: 5, Total Step: 4936, Loss: 0.260009765625
Epoch: 5, Total Step: 4946, Loss: 0.253662109375
Epoch: 5, Total Step: 4956, Loss: 0.41943359375
[2023-10-12 16:03:19,756] [INFO] [logging.py:96:log_dist] [Rank 0] step=1240, skipped=21, lr=[0.00018872377029164993], mom=[(0.9, 0.95)]
[2023-10-12 16:03:19,757] [INFO] [timer.py:260:stop] epoch=5/micro_step=285/global_step=1240, RunningAvgSamplesPerSec=2.22965958710334, CurrSamplesPerSec=2.5374209377333736, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 4966, Loss: 0.1868896484375
Epoch: 5, Total Step: 4976, Loss: 0.253173828125
Epoch: 5, Total Step: 4986, Loss: 0.2496337890625
Epoch: 5, Total Step: 4996, Loss: 0.3154296875
[2023-10-12 16:05:25,376] [INFO] [logging.py:96:log_dist] [Rank 0] step=1250, skipped=21, lr=[0.0001851151436409992], mom=[(0.9, 0.95)]
[2023-10-12 16:05:25,376] [INFO] [timer.py:260:stop] epoch=5/micro_step=325/global_step=1250, RunningAvgSamplesPerSec=2.2318995691027506, CurrSamplesPerSec=2.4996962953546022, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 5006, Loss: 0.264892578125
Epoch: 5, Total Step: 5016, Loss: 0.2208251953125
Epoch: 5, Total Step: 5026, Loss: 0.2313232421875
Epoch: 5, Total Step: 5036, Loss: 0.19921875
[2023-10-12 16:07:32,517] [INFO] [logging.py:96:log_dist] [Rank 0] step=1260, skipped=21, lr=[0.00018152095536496187], mom=[(0.9, 0.95)]
[2023-10-12 16:07:32,517] [INFO] [timer.py:260:stop] epoch=5/micro_step=365/global_step=1260, RunningAvgSamplesPerSec=2.2339192080974133, CurrSamplesPerSec=2.5260330157463993, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 5046, Loss: 0.2384033203125
Epoch: 5, Total Step: 5056, Loss: 0.2958984375
Epoch: 5, Total Step: 5066, Loss: 0.291259765625
Epoch: 5, Total Step: 5076, Loss: 0.267822265625
[2023-10-12 16:09:38,237] [INFO] [logging.py:96:log_dist] [Rank 0] step=1270, skipped=21, lr=[0.00017794200525319103], mom=[(0.9, 0.95)]
[2023-10-12 16:09:38,238] [INFO] [timer.py:260:stop] epoch=5/micro_step=405/global_step=1270, RunningAvgSamplesPerSec=2.236085433950699, CurrSamplesPerSec=2.5276831470869636, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 5086, Loss: 0.306640625
Epoch: 5, Total Step: 5096, Loss: 0.281005859375
Epoch: 5, Total Step: 5106, Loss: 0.262451171875
Epoch: 5, Total Step: 5116, Loss: 0.206787109375
[2023-10-12 16:11:40,806] [INFO] [logging.py:96:log_dist] [Rank 0] step=1280, skipped=21, lr=[0.00017437908970449728], mom=[(0.9, 0.95)]
[2023-10-12 16:11:40,806] [INFO] [timer.py:260:stop] epoch=5/micro_step=445/global_step=1280, RunningAvgSamplesPerSec=2.238608923714907, CurrSamplesPerSec=2.641120786114121, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 5126, Loss: 0.250244140625
Epoch: 5, Total Step: 5136, Loss: 0.27685546875
Epoch: 5, Total Step: 5146, Loss: 0.2137451171875
Epoch: 5, Total Step: 5156, Loss: 0.166015625
[2023-10-12 16:13:44,759] [INFO] [logging.py:96:log_dist] [Rank 0] step=1290, skipped=21, lr=[0.00017083300154963193], mom=[(0.9, 0.95)]
[2023-10-12 16:13:44,760] [INFO] [timer.py:260:stop] epoch=5/micro_step=485/global_step=1290, RunningAvgSamplesPerSec=2.240929446872453, CurrSamplesPerSec=2.579289968755895, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 5166, Loss: 0.2822265625
Epoch: 5, Total Step: 5176, Loss: 0.2205810546875
Epoch: 5, Total Step: 5186, Loss: 0.3369140625
Epoch: 5, Total Step: 5196, Loss: 0.15283203125
[2023-10-12 16:16:24,848] [INFO] [logging.py:96:log_dist] [Rank 0] step=1300, skipped=21, lr=[0.00016730452987486345], mom=[(0.9, 0.95)]
[2023-10-12 16:16:24,849] [INFO] [timer.py:260:stop] epoch=5/micro_step=525/global_step=1300, RunningAvgSamplesPerSec=2.2388516099359848, CurrSamplesPerSec=2.590126483756359, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 5206, Loss: 0.2467041015625
Epoch: 5, Total Step: 5216, Loss: 0.2183837890625
Epoch: 5, Total Step: 5226, Loss: 0.2607421875
Epoch: 5, Total Step: 5236, Loss: 0.21728515625
[2023-10-12 16:20:43,228] [INFO] [logging.py:96:log_dist] [Rank 0] step=1310, skipped=21, lr=[0.00016379445984638803], mom=[(0.9, 0.95)]
[2023-10-12 16:20:43,229] [INFO] [timer.py:260:stop] epoch=5/micro_step=565/global_step=1310, RunningAvgSamplesPerSec=2.225123780349627, CurrSamplesPerSec=1.0922685244476042, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 5246, Loss: 0.2000732421875
Epoch: 5, Total Step: 5256, Loss: 0.25244140625
Epoch: 5, Total Step: 5266, Loss: 0.261962890625
Epoch: 5, Total Step: 5276, Loss: 0.327880859375
[2023-10-12 16:23:25,195] [INFO] [logging.py:96:log_dist] [Rank 0] step=1320, skipped=21, lr=[0.00016030357253561173], mom=[(0.9, 0.95)]
[2023-10-12 16:23:25,195] [INFO] [timer.py:260:stop] epoch=5/micro_step=605/global_step=1320, RunningAvgSamplesPerSec=2.2230035496314455, CurrSamplesPerSec=2.5752035283686823, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 5286, Loss: 0.219970703125
Epoch: 5, Total Step: 5296, Loss: 0.253173828125
[2023-10-12 16:24:14,989] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, but hysteresis is 2. Reducing hysteresis to 1
[2023-10-12 16:24:27,467] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
Epoch: 5, Total Step: 5306, Loss: 0.30419921875
Epoch: 5, Total Step: 5316, Loss: 0.1942138671875
[2023-10-12 16:25:29,664] [INFO] [logging.py:96:log_dist] [Rank 0] step=1330, skipped=23, lr=[0.000157525196403066], mom=[(0.9, 0.95)]
[2023-10-12 16:25:29,665] [INFO] [timer.py:260:stop] epoch=5/micro_step=645/global_step=1330, RunningAvgSamplesPerSec=2.2252799123311413, CurrSamplesPerSec=2.5853479866603646, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 5326, Loss: 0.36669921875
Epoch: 5, Total Step: 5336, Loss: 0.269287109375
Epoch: 5, Total Step: 5346, Loss: 0.1912841796875
Epoch: 5, Total Step: 5356, Loss: 0.251220703125
[2023-10-12 16:27:33,835] [INFO] [logging.py:96:log_dist] [Rank 0] step=1340, skipped=23, lr=[0.00015407079254835506], mom=[(0.9, 0.95)]
[2023-10-12 16:27:33,836] [INFO] [timer.py:260:stop] epoch=5/micro_step=685/global_step=1340, RunningAvgSamplesPerSec=2.227562384923062, CurrSamplesPerSec=2.556834339380794, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 5366, Loss: 0.2783203125
Epoch: 5, Total Step: 5376, Loss: 0.341796875
Epoch: 5, Total Step: 5386, Loss: 0.215576171875
Epoch: 5, Total Step: 5396, Loss: 0.302490234375
[2023-10-12 16:29:38,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=1350, skipped=23, lr=[0.0001506377351512195], mom=[(0.9, 0.95)]
[2023-10-12 16:29:38,361] [INFO] [timer.py:260:stop] epoch=5/micro_step=725/global_step=1350, RunningAvgSamplesPerSec=2.2297742034641823, CurrSamplesPerSec=2.5900392644017316, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 5406, Loss: 0.281494140625
Epoch: 5, Total Step: 5416, Loss: 0.26513671875
Epoch: 5, Total Step: 5426, Loss: 0.276611328125
Epoch: 5, Total Step: 5436, Loss: 0.311767578125
[2023-10-12 16:31:42,563] [INFO] [logging.py:96:log_dist] [Rank 0] step=1360, skipped=23, lr=[0.00014722678814598153], mom=[(0.9, 0.95)]
[2023-10-12 16:31:42,563] [INFO] [timer.py:260:stop] epoch=5/micro_step=765/global_step=1360, RunningAvgSamplesPerSec=2.2319943424356405, CurrSamplesPerSec=2.527712994593217, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 5446, Loss: 0.22314453125
Epoch: 5, Total Step: 5456, Loss: 0.267822265625
Epoch: 5, Total Step: 5466, Loss: 0.2037353515625
Epoch: 5, Total Step: 5476, Loss: 0.3720703125
[2023-10-12 16:33:49,959] [INFO] [logging.py:96:log_dist] [Rank 0] step=1370, skipped=23, lr=[0.00014383871054689213], mom=[(0.9, 0.95)]
[2023-10-12 16:33:49,959] [INFO] [timer.py:260:stop] epoch=5/micro_step=805/global_step=1370, RunningAvgSamplesPerSec=2.2338237473708675, CurrSamplesPerSec=2.5111258490862283, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 5486, Loss: 0.16943359375
Epoch: 5, Total Step: 5496, Loss: 0.265869140625
Epoch: 5, Total Step: 5506, Loss: 0.23876953125
Epoch: 5, Total Step: 5516, Loss: 0.222900390625
[2023-10-12 16:35:52,654] [INFO] [logging.py:96:log_dist] [Rank 0] step=1380, skipped=23, lr=[0.00014047425627923264], mom=[(0.9, 0.95)]
[2023-10-12 16:35:52,654] [INFO] [timer.py:260:stop] epoch=5/micro_step=845/global_step=1380, RunningAvgSamplesPerSec=2.236162637953928, CurrSamplesPerSec=2.671308849251442, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 5526, Loss: 0.204345703125
Epoch: 5, Total Step: 5536, Loss: 0.2384033203125
Epoch: 5, Total Step: 5546, Loss: 0.2314453125
Epoch: 5, Total Step: 5556, Loss: 0.22412109375
[2023-10-12 16:37:54,855] [INFO] [logging.py:96:log_dist] [Rank 0] step=1390, skipped=23, lr=[0.00013713417401154955], mom=[(0.9, 0.95)]
[2023-10-12 16:37:54,856] [INFO] [timer.py:260:stop] epoch=5/micro_step=885/global_step=1390, RunningAvgSamplesPerSec=2.238528213907837, CurrSamplesPerSec=2.5526938603345095, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 5566, Loss: 0.1632080078125
Epoch: 5, Total Step: 5576, Loss: 0.256591796875
Epoch: 5, Total Step: 5586, Loss: 0.287109375
Epoch: 5, Total Step: 5596, Loss: 0.144775390625
[2023-10-12 16:39:58,792] [INFO] [logging.py:96:log_dist] [Rank 0] step=1400, skipped=23, lr=[0.00013381920698905787], mom=[(0.9, 0.95)]
[2023-10-12 16:39:58,792] [INFO] [timer.py:260:stop] epoch=5/micro_step=925/global_step=1400, RunningAvgSamplesPerSec=2.240671133222796, CurrSamplesPerSec=2.6624173063624954, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 5, Total Step: 5606, Loss: 0.2347412109375
***** Evaluating perplexity, Epoch 6/9 *****
Invalidate trace cache @ step 0: expected module 0, but got module 6
ppl: 1.3815252780914307
eval loss: 0.32318371534347534
Beginning of Epoch 7/9, Total Micro Batches 935
Epoch: 6, Total Step: 5611, Loss: 0.1842041015625
Epoch: 6, Total Step: 5621, Loss: 0.25048828125
Epoch: 6, Total Step: 5631, Loss: 0.28466796875
[2023-10-12 16:50:01,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=1410, skipped=23, lr=[0.0001305300928682523], mom=[(0.9, 0.95)]
[2023-10-12 16:50:01,322] [INFO] [timer.py:260:stop] epoch=6/micro_step=30/global_step=1410, RunningAvgSamplesPerSec=2.2385339257853367, CurrSamplesPerSec=2.537421753233845, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 5641, Loss: 0.259765625
Epoch: 6, Total Step: 5651, Loss: 0.4150390625
Epoch: 6, Total Step: 5661, Loss: 0.2467041015625
Epoch: 6, Total Step: 5671, Loss: 0.264892578125
[2023-10-12 16:52:07,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=1420, skipped=23, lr=[0.00012726756355276164], mom=[(0.9, 0.95)]
[2023-10-12 16:52:07,780] [INFO] [timer.py:260:stop] epoch=6/micro_step=70/global_step=1420, RunningAvgSamplesPerSec=2.240366953366984, CurrSamplesPerSec=2.4829171777526113, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 5681, Loss: 0.2110595703125
Epoch: 6, Total Step: 5691, Loss: 0.189697265625
Epoch: 6, Total Step: 5701, Loss: 0.385009765625
[2023-10-12 16:53:25,234] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, but hysteresis is 2. Reducing hysteresis to 1
[2023-10-12 16:53:38,006] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
Epoch: 6, Total Step: 5711, Loss: 0.34765625
[2023-10-12 16:54:15,791] [INFO] [logging.py:96:log_dist] [Rank 0] step=1430, skipped=25, lr=[0.00012467716922696356], mom=[(0.9, 0.95)]
[2023-10-12 16:54:15,792] [INFO] [timer.py:260:stop] epoch=6/micro_step=110/global_step=1430, RunningAvgSamplesPerSec=2.242004793968631, CurrSamplesPerSec=2.549367198437729, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 5721, Loss: 0.253173828125
Epoch: 6, Total Step: 5731, Loss: 0.25244140625
Epoch: 6, Total Step: 5741, Loss: 0.285400390625
Epoch: 6, Total Step: 5751, Loss: 0.23583984375
[2023-10-12 16:56:19,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=1440, skipped=25, lr=[0.00012146431797200256], mom=[(0.9, 0.95)]
[2023-10-12 16:56:19,915] [INFO] [timer.py:260:stop] epoch=6/micro_step=150/global_step=1440, RunningAvgSamplesPerSec=2.244047101628056, CurrSamplesPerSec=2.6414919679434505, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 5761, Loss: 0.2178955078125
Epoch: 6, Total Step: 5771, Loss: 0.207763671875
Epoch: 6, Total Step: 5781, Loss: 0.2493896484375
Epoch: 6, Total Step: 5791, Loss: 0.26904296875
[2023-10-12 16:58:26,716] [INFO] [logging.py:96:log_dist] [Rank 0] step=1450, skipped=25, lr=[0.00011828006886602421], mom=[(0.9, 0.95)]
[2023-10-12 16:58:26,716] [INFO] [timer.py:260:stop] epoch=6/micro_step=190/global_step=1450, RunningAvgSamplesPerSec=2.2457743932870957, CurrSamplesPerSec=2.452298013151863, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 5801, Loss: 0.25146484375
Epoch: 6, Total Step: 5811, Loss: 0.3046875
Epoch: 6, Total Step: 5821, Loss: 0.25732421875
Epoch: 6, Total Step: 5831, Loss: 0.4169921875
[2023-10-12 17:00:33,373] [INFO] [logging.py:96:log_dist] [Rank 0] step=1460, skipped=25, lr=[0.00011512513047777689], mom=[(0.9, 0.95)]
[2023-10-12 17:00:33,374] [INFO] [timer.py:260:stop] epoch=6/micro_step=230/global_step=1460, RunningAvgSamplesPerSec=2.2474949941651525, CurrSamplesPerSec=2.570281852970572, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 5841, Loss: 0.1778564453125
Epoch: 6, Total Step: 5851, Loss: 0.2646484375
Epoch: 6, Total Step: 5861, Loss: 0.31005859375
Epoch: 6, Total Step: 5871, Loss: 0.251953125
[2023-10-12 17:02:37,073] [INFO] [logging.py:96:log_dist] [Rank 0] step=1470, skipped=25, lr=[0.00011200020485369927], mom=[(0.9, 0.95)]
[2023-10-12 17:02:37,074] [INFO] [timer.py:260:stop] epoch=6/micro_step=270/global_step=1470, RunningAvgSamplesPerSec=2.2495143860902713, CurrSamplesPerSec=2.5898639437812103, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 5881, Loss: 0.2440185546875
Epoch: 6, Total Step: 5891, Loss: 0.403564453125
Epoch: 6, Total Step: 5901, Loss: 0.1807861328125
Epoch: 6, Total Step: 5911, Loss: 0.2447509765625
[2023-10-12 17:04:41,082] [INFO] [logging.py:96:log_dist] [Rank 0] step=1480, skipped=25, lr=[0.00010890598736169879], mom=[(0.9, 0.95)]
[2023-10-12 17:04:41,083] [INFO] [timer.py:260:stop] epoch=6/micro_step=310/global_step=1480, RunningAvgSamplesPerSec=2.2514757544374326, CurrSamplesPerSec=2.5740139932688644, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 5921, Loss: 0.24072265625
Epoch: 6, Total Step: 5931, Loss: 0.3056640625
Epoch: 6, Total Step: 5941, Loss: 0.256103515625
Epoch: 6, Total Step: 5951, Loss: 0.2147216796875
[2023-10-12 17:08:33,253] [INFO] [logging.py:96:log_dist] [Rank 0] step=1490, skipped=25, lr=[0.00010584316653641676], mom=[(0.9, 0.95)]
[2023-10-12 17:08:33,255] [INFO] [timer.py:260:stop] epoch=6/micro_step=350/global_step=1490, RunningAvgSamplesPerSec=2.24194171565269, CurrSamplesPerSec=1.1175230234913194, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 5961, Loss: 0.2242431640625
Epoch: 6, Total Step: 5971, Loss: 0.19384765625
Epoch: 6, Total Step: 5981, Loss: 0.2327880859375
Epoch: 6, Total Step: 5991, Loss: 0.28759765625
[2023-10-12 17:12:24,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=1500, skipped=25, lr=[0.00010281242392601276], mom=[(0.9, 0.95)]
[2023-10-12 17:12:24,327] [INFO] [timer.py:260:stop] epoch=6/micro_step=390/global_step=1500, RunningAvgSamplesPerSec=2.232731133967455, CurrSamplesPerSec=1.9082856858094193, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 6001, Loss: 0.281494140625
Epoch: 6, Total Step: 6011, Loss: 0.259033203125
Epoch: 6, Total Step: 6021, Loss: 0.2998046875
Epoch: 6, Total Step: 6031, Loss: 0.2734375
[2023-10-12 17:14:28,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=1510, skipped=25, lr=[9.981443394050524e-05], mom=[(0.9, 0.95)]
[2023-10-12 17:14:28,813] [INFO] [timer.py:260:stop] epoch=6/micro_step=430/global_step=1510, RunningAvgSamplesPerSec=2.234685657350021, CurrSamplesPerSec=2.571581948193108, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 6041, Loss: 0.255126953125
Epoch: 6, Total Step: 6051, Loss: 0.20068359375
Epoch: 6, Total Step: 6061, Loss: 0.24267578125
Epoch: 6, Total Step: 6071, Loss: 0.26806640625
[2023-10-12 17:16:33,601] [INFO] [logging.py:96:log_dist] [Rank 0] step=1520, skipped=25, lr=[9.684986370169929e-05], mom=[(0.9, 0.95)]
[2023-10-12 17:16:33,601] [INFO] [timer.py:260:stop] epoch=6/micro_step=470/global_step=1520, RunningAvgSamplesPerSec=2.236587399965676, CurrSamplesPerSec=2.5683371218637463, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 6081, Loss: 0.21044921875
Epoch: 6, Total Step: 6091, Loss: 0.1627197265625
Epoch: 6, Total Step: 6101, Loss: 0.272705078125
Epoch: 6, Total Step: 6111, Loss: 0.2152099609375
[2023-10-12 17:18:16,154] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, but hysteresis is 2. Reducing hysteresis to 1
[2023-10-12 17:18:28,952] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
[2023-10-12 17:18:41,695] [INFO] [logging.py:96:log_dist] [Rank 0] step=1530, skipped=27, lr=[9.450271329032403e-05], mom=[(0.9, 0.95)]
[2023-10-12 17:18:41,696] [INFO] [timer.py:260:stop] epoch=6/micro_step=510/global_step=1530, RunningAvgSamplesPerSec=2.2381284242034765, CurrSamplesPerSec=2.512390758663131, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 6121, Loss: 0.32958984375
Epoch: 6, Total Step: 6131, Loss: 0.1484375
Epoch: 6, Total Step: 6141, Loss: 0.2384033203125
Epoch: 6, Total Step: 6151, Loss: 0.2125244140625
[2023-10-12 17:20:50,058] [INFO] [logging.py:96:log_dist] [Rank 0] step=1540, skipped=27, lr=[9.159995591094383e-05], mom=[(0.9, 0.95)]
[2023-10-12 17:20:50,059] [INFO] [timer.py:260:stop] epoch=6/micro_step=550/global_step=1540, RunningAvgSamplesPerSec=2.2396235509541595, CurrSamplesPerSec=2.4970948222973095, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 6161, Loss: 0.2529296875
Epoch: 6, Total Step: 6171, Loss: 0.2098388671875
Epoch: 6, Total Step: 6181, Loss: 0.1932373046875
Epoch: 6, Total Step: 6191, Loss: 0.2442626953125
[2023-10-12 17:22:58,639] [INFO] [logging.py:96:log_dist] [Rank 0] step=1550, skipped=27, lr=[8.873244618877766e-05], mom=[(0.9, 0.95)]
[2023-10-12 17:22:58,639] [INFO] [timer.py:260:stop] epoch=6/micro_step=590/global_step=1550, RunningAvgSamplesPerSec=2.2410795316813874, CurrSamplesPerSec=2.503349895847728, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 6201, Loss: 0.253173828125
Epoch: 6, Total Step: 6211, Loss: 0.3193359375
Epoch: 6, Total Step: 6221, Loss: 0.2149658203125
Epoch: 6, Total Step: 6231, Loss: 0.255859375
[2023-10-12 17:25:07,359] [INFO] [logging.py:96:log_dist] [Rank 0] step=1560, skipped=27, lr=[8.590082221076764e-05], mom=[(0.9, 0.95)]
[2023-10-12 17:25:07,359] [INFO] [timer.py:260:stop] epoch=6/micro_step=630/global_step=1560, RunningAvgSamplesPerSec=2.2425041480947603, CurrSamplesPerSec=2.492553282803212, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 6241, Loss: 0.29541015625
Epoch: 6, Total Step: 6251, Loss: 0.189208984375
Epoch: 6, Total Step: 6261, Loss: 0.3564453125
Epoch: 6, Total Step: 6271, Loss: 0.262451171875
[2023-10-12 17:27:15,156] [INFO] [logging.py:96:log_dist] [Rank 0] step=1570, skipped=27, lr=[8.31057140784513e-05], mom=[(0.9, 0.95)]
[2023-10-12 17:27:15,157] [INFO] [timer.py:260:stop] epoch=6/micro_step=670/global_step=1570, RunningAvgSamplesPerSec=2.244005402894611, CurrSamplesPerSec=2.5005370787002503, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 6281, Loss: 0.1871337890625
Epoch: 6, Total Step: 6291, Loss: 0.2437744140625
Epoch: 6, Total Step: 6301, Loss: 0.273193359375
Epoch: 6, Total Step: 6311, Loss: 0.3349609375
[2023-10-12 17:29:23,837] [INFO] [logging.py:96:log_dist] [Rank 0] step=1580, skipped=27, lr=[8.034774376774972e-05], mom=[(0.9, 0.95)]
[2023-10-12 17:29:23,837] [INFO] [timer.py:260:stop] epoch=6/micro_step=710/global_step=1580, RunningAvgSamplesPerSec=2.2454018115003347, CurrSamplesPerSec=2.4899057212831903, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 6321, Loss: 0.209716796875
Epoch: 6, Total Step: 6331, Loss: 0.293701171875
Epoch: 6, Total Step: 6341, Loss: 0.275146484375
Epoch: 6, Total Step: 6351, Loss: 0.259521484375
[2023-10-12 17:31:56,626] [INFO] [logging.py:96:log_dist] [Rank 0] step=1590, skipped=27, lr=[7.762752499056358e-05], mom=[(0.9, 0.95)]
[2023-10-12 17:31:56,628] [INFO] [timer.py:260:stop] epoch=6/micro_step=750/global_step=1590, RunningAvgSamplesPerSec=2.2443922374607035, CurrSamplesPerSec=0.9278717068102526, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 6361, Loss: 0.27001953125
Epoch: 6, Total Step: 6371, Loss: 0.303955078125
Epoch: 6, Total Step: 6381, Loss: 0.2183837890625
Epoch: 6, Total Step: 6391, Loss: 0.2587890625
[2023-10-12 17:36:09,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=1600, skipped=27, lr=[7.494566305820788e-05], mom=[(0.9, 0.95)]
[2023-10-12 17:36:09,898] [INFO] [timer.py:260:stop] epoch=6/micro_step=790/global_step=1600, RunningAvgSamplesPerSec=2.2335515387107074, CurrSamplesPerSec=1.8083883016827014, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 6401, Loss: 0.1968994140625
Epoch: 6, Total Step: 6411, Loss: 0.36376953125
Epoch: 6, Total Step: 6421, Loss: 0.1656494140625
Epoch: 6, Total Step: 6431, Loss: 0.259033203125
[2023-10-12 17:39:24,719] [INFO] [logging.py:96:log_dist] [Rank 0] step=1610, skipped=27, lr=[7.230275474671674e-05], mom=[(0.9, 0.95)]
[2023-10-12 17:39:24,719] [INFO] [timer.py:260:stop] epoch=6/micro_step=830/global_step=1610, RunningAvgSamplesPerSec=2.2285751502706486, CurrSamplesPerSec=2.4923382420588287, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 6441, Loss: 0.2298583984375
Epoch: 6, Total Step: 6451, Loss: 0.2186279296875
Epoch: 6, Total Step: 6461, Loss: 0.2010498046875
Epoch: 6, Total Step: 6471, Loss: 0.2340087890625
[2023-10-12 17:41:33,473] [INFO] [logging.py:96:log_dist] [Rank 0] step=1620, skipped=27, lr=[6.96993881640464e-05], mom=[(0.9, 0.95)]
[2023-10-12 17:41:33,473] [INFO] [timer.py:260:stop] epoch=6/micro_step=870/global_step=1620, RunningAvgSamplesPerSec=2.2300057854992006, CurrSamplesPerSec=2.4797115076006975, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 6481, Loss: 0.228759765625
Epoch: 6, Total Step: 6491, Loss: 0.2197265625
Epoch: 6, Total Step: 6501, Loss: 0.160888671875
Epoch: 6, Total Step: 6511, Loss: 0.2509765625
[2023-10-12 17:43:42,338] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, but hysteresis is 2. Reducing hysteresis to 1
[2023-10-12 17:43:42,339] [INFO] [logging.py:96:log_dist] [Rank 0] step=1630, skipped=28, lr=[6.739064540057424e-05], mom=[(0.9, 0.95)]
[2023-10-12 17:43:42,339] [INFO] [timer.py:260:stop] epoch=6/micro_step=910/global_step=1630, RunningAvgSamplesPerSec=2.231409800005846, CurrSamplesPerSec=2.5060090622272075, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 6, Total Step: 6521, Loss: 0.279541015625
[2023-10-12 17:43:55,161] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
Epoch: 6, Total Step: 6531, Loss: 0.1402587890625
Epoch: 6, Total Step: 6541, Loss: 0.22705078125
***** Evaluating perplexity, Epoch 7/9 *****
Invalidate trace cache @ step 0: expected module 0, but got module 6
ppl: 1.3869812488555908
eval loss: 0.3271251320838928
Beginning of Epoch 8/9, Total Micro Batches 935
Epoch: 7, Total Step: 6546, Loss: 0.1790771484375
Epoch: 7, Total Step: 6556, Loss: 0.2415771484375
[2023-10-12 17:50:27,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=1640, skipped=29, lr=[6.511481692994075e-05], mom=[(0.9, 0.95)]
[2023-10-12 17:50:27,361] [INFO] [timer.py:260:stop] epoch=7/micro_step=15/global_step=1640, RunningAvgSamplesPerSec=2.2324791077788424, CurrSamplesPerSec=2.487733000008786, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 6566, Loss: 0.2763671875
Epoch: 7, Total Step: 6576, Loss: 0.247802734375
Epoch: 7, Total Step: 6586, Loss: 0.4033203125
Epoch: 7, Total Step: 6596, Loss: 0.2423095703125
[2023-10-12 17:52:36,472] [INFO] [logging.py:96:log_dist] [Rank 0] step=1650, skipped=29, lr=[6.262522053311665e-05], mom=[(0.9, 0.95)]
[2023-10-12 17:52:36,473] [INFO] [timer.py:260:stop] epoch=7/micro_step=55/global_step=1650, RunningAvgSamplesPerSec=2.233829939270084, CurrSamplesPerSec=2.478408297959282, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 6606, Loss: 0.257080078125
Epoch: 7, Total Step: 6616, Loss: 0.205810546875
Epoch: 7, Total Step: 6626, Loss: 0.1849365234375
Epoch: 7, Total Step: 6636, Loss: 0.38818359375
[2023-10-12 17:54:45,902] [INFO] [logging.py:96:log_dist] [Rank 0] step=1660, skipped=29, lr=[6.0177319339315974e-05], mom=[(0.9, 0.95)]
[2023-10-12 17:54:45,903] [INFO] [timer.py:260:stop] epoch=7/micro_step=95/global_step=1660, RunningAvgSamplesPerSec=2.2351362560304904, CurrSamplesPerSec=2.4796579529040654, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 6646, Loss: 0.341552734375
Epoch: 7, Total Step: 6656, Loss: 0.2476806640625
Epoch: 7, Total Step: 6666, Loss: 0.2454833984375
Epoch: 7, Total Step: 6676, Loss: 0.28076171875
[2023-10-12 17:56:56,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=1670, skipped=29, lr=[5.777165806292109e-05], mom=[(0.9, 0.95)]
[2023-10-12 17:56:56,263] [INFO] [timer.py:260:stop] epoch=7/micro_step=135/global_step=1670, RunningAvgSamplesPerSec=2.236341983880411, CurrSamplesPerSec=2.4546034331928097, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 6686, Loss: 0.2308349609375
Epoch: 7, Total Step: 6696, Loss: 0.21240234375
Epoch: 7, Total Step: 6706, Loss: 0.2022705078125
Epoch: 7, Total Step: 6716, Loss: 0.2449951171875
[2023-10-12 18:01:06,012] [INFO] [logging.py:96:log_dist] [Rank 0] step=1680, skipped=29, lr=[5.540877201896e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:01:06,012] [INFO] [timer.py:260:stop] epoch=7/micro_step=175/global_step=1680, RunningAvgSamplesPerSec=2.226463164062398, CurrSamplesPerSec=1.0884278150676803, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 6726, Loss: 0.26318359375
Epoch: 7, Total Step: 6736, Loss: 0.2447509765625
Epoch: 7, Total Step: 6746, Loss: 0.2958984375
Epoch: 7, Total Step: 6756, Loss: 0.249755859375
[2023-10-12 18:04:40,913] [INFO] [logging.py:96:log_dist] [Rank 0] step=1690, skipped=29, lr=[5.3089187003986514e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:04:40,914] [INFO] [timer.py:260:stop] epoch=7/micro_step=215/global_step=1690, RunningAvgSamplesPerSec=2.219961250328535, CurrSamplesPerSec=2.4935759976967007, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 6766, Loss: 0.406982421875
Epoch: 7, Total Step: 6776, Loss: 0.1748046875
Epoch: 7, Total Step: 6786, Loss: 0.2578125
Epoch: 7, Total Step: 6796, Loss: 0.30224609375
[2023-10-12 18:06:51,226] [INFO] [logging.py:96:log_dist] [Rank 0] step=1700, skipped=29, lr=[5.081341917907853e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:06:51,227] [INFO] [timer.py:260:stop] epoch=7/micro_step=255/global_step=1700, RunningAvgSamplesPerSec=2.2212232748668272, CurrSamplesPerSec=2.447239338023499, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 6806, Loss: 0.245849609375
Epoch: 7, Total Step: 6816, Loss: 0.2381591796875
Epoch: 7, Total Step: 6826, Loss: 0.39306640625
Epoch: 7, Total Step: 6836, Loss: 0.1778564453125
[2023-10-12 18:09:02,166] [INFO] [logging.py:96:log_dist] [Rank 0] step=1710, skipped=29, lr=[4.858197495498001e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:09:02,167] [INFO] [timer.py:260:stop] epoch=7/micro_step=295/global_step=1710, RunningAvgSamplesPerSec=2.2224144490032915, CurrSamplesPerSec=2.4510965455097367, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 6846, Loss: 0.237548828125
Epoch: 7, Total Step: 6856, Loss: 0.2354736328125
Epoch: 7, Total Step: 6866, Loss: 0.2978515625
Epoch: 7, Total Step: 6876, Loss: 0.25048828125
[2023-10-12 18:11:13,965] [INFO] [logging.py:96:log_dist] [Rank 0] step=1720, skipped=29, lr=[4.6395350879413434e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:11:13,966] [INFO] [timer.py:260:stop] epoch=7/micro_step=335/global_step=1720, RunningAvgSamplesPerSec=2.223516440803016, CurrSamplesPerSec=2.416897623673306, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 6886, Loss: 0.21044921875
Epoch: 7, Total Step: 6896, Loss: 0.2193603515625
Epoch: 7, Total Step: 6906, Loss: 0.18994140625
Epoch: 7, Total Step: 6916, Loss: 0.228271484375
[2023-10-12 18:13:25,235] [INFO] [logging.py:96:log_dist] [Rank 0] step=1730, skipped=29, lr=[4.425403352658591e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:13:25,236] [INFO] [timer.py:260:stop] epoch=7/micro_step=375/global_step=1730, RunningAvgSamplesPerSec=2.224653313418492, CurrSamplesPerSec=2.462056511628773, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 6926, Loss: 0.282958984375
[2023-10-12 18:13:51,351] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, but hysteresis is 2. Reducing hysteresis to 1
[2023-10-12 18:14:04,368] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
Epoch: 7, Total Step: 6936, Loss: 0.275390625
Epoch: 7, Total Step: 6946, Loss: 0.253173828125
Epoch: 7, Total Step: 6956, Loss: 0.294921875
[2023-10-12 18:15:35,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=1740, skipped=31, lr=[4.257392102906823e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:15:35,841] [INFO] [timer.py:260:stop] epoch=7/micro_step=415/global_step=1740, RunningAvgSamplesPerSec=2.2258385003690155, CurrSamplesPerSec=2.4136452281397314, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 6966, Loss: 0.269287109375
Epoch: 7, Total Step: 6976, Loss: 0.2509765625
Epoch: 7, Total Step: 6986, Loss: 0.1978759765625
Epoch: 7, Total Step: 6996, Loss: 0.2384033203125
[2023-10-12 18:17:44,233] [INFO] [logging.py:96:log_dist] [Rank 0] step=1750, skipped=31, lr=[4.0515349695354726e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:17:44,234] [INFO] [timer.py:260:stop] epoch=7/micro_step=455/global_step=1750, RunningAvgSamplesPerSec=2.2272074950382925, CurrSamplesPerSec=2.5476042102501104, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 7006, Loss: 0.261962890625
Epoch: 7, Total Step: 7016, Loss: 0.2056884765625
Epoch: 7, Total Step: 7026, Loss: 0.159423828125
Epoch: 7, Total Step: 7036, Loss: 0.266845703125
[2023-10-12 18:19:50,128] [INFO] [logging.py:96:log_dist] [Rank 0] step=1760, skipped=31, lr=[3.850339352003973e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:19:50,128] [INFO] [timer.py:260:stop] epoch=7/micro_step=495/global_step=1760, RunningAvgSamplesPerSec=2.2287826925422283, CurrSamplesPerSec=2.5380111613492367, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 7046, Loss: 0.21337890625
Epoch: 7, Total Step: 7056, Loss: 0.323486328125
Epoch: 7, Total Step: 7066, Loss: 0.1468505859375
Epoch: 7, Total Step: 7076, Loss: 0.23291015625
[2023-10-12 18:21:56,625] [INFO] [logging.py:96:log_dist] [Rank 0] step=1770, skipped=31, lr=[3.653850020970073e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:21:56,625] [INFO] [timer.py:260:stop] epoch=7/micro_step=535/global_step=1770, RunningAvgSamplesPerSec=2.2302902172051193, CurrSamplesPerSec=2.5437278714719085, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 7086, Loss: 0.20751953125
Epoch: 7, Total Step: 7096, Loss: 0.248046875
Epoch: 7, Total Step: 7106, Loss: 0.2054443359375
Epoch: 7, Total Step: 7116, Loss: 0.190185546875
[2023-10-12 18:24:59,141] [INFO] [logging.py:96:log_dist] [Rank 0] step=1780, skipped=31, lr=[3.462110699834392e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:24:59,141] [INFO] [timer.py:260:stop] epoch=7/micro_step=575/global_step=1780, RunningAvgSamplesPerSec=2.226890460495227, CurrSamplesPerSec=1.074190027640653, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 7126, Loss: 0.240234375
Epoch: 7, Total Step: 7136, Loss: 0.24658203125
Epoch: 7, Total Step: 7146, Loss: 0.313720703125
Epoch: 7, Total Step: 7156, Loss: 0.2099609375
[2023-10-12 18:29:28,615] [INFO] [logging.py:96:log_dist] [Rank 0] step=1790, skipped=31, lr=[3.275164055010974e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:29:28,618] [INFO] [timer.py:260:stop] epoch=7/micro_step=615/global_step=1790, RunningAvgSamplesPerSec=2.2160515017283835, CurrSamplesPerSec=1.631778786672302, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 7166, Loss: 0.2486572265625
Epoch: 7, Total Step: 7176, Loss: 0.2900390625
Epoch: 7, Total Step: 7186, Loss: 0.18603515625
Epoch: 7, Total Step: 7196, Loss: 0.349609375
[2023-10-12 18:31:48,121] [INFO] [logging.py:96:log_dist] [Rank 0] step=1800, skipped=31, lr=[3.0930516864330236e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:31:48,121] [INFO] [timer.py:260:stop] epoch=7/micro_step=655/global_step=1800, RunningAvgSamplesPerSec=2.216477438858163, CurrSamplesPerSec=2.5974676061935575, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 7206, Loss: 0.2587890625
Epoch: 7, Total Step: 7216, Loss: 0.18408203125
Epoch: 7, Total Step: 7226, Loss: 0.2393798828125
Epoch: 7, Total Step: 7236, Loss: 0.2685546875
[2023-10-12 18:33:53,429] [INFO] [logging.py:96:log_dist] [Rank 0] step=1810, skipped=31, lr=[2.9158141182959645e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:33:53,430] [INFO] [timer.py:260:stop] epoch=7/micro_step=695/global_step=1810, RunningAvgSamplesPerSec=2.218103689260899, CurrSamplesPerSec=2.4887728982989, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 7246, Loss: 0.32861328125
Epoch: 7, Total Step: 7256, Loss: 0.2039794921875
Epoch: 7, Total Step: 7266, Loss: 0.286865234375
Epoch: 7, Total Step: 7276, Loss: 0.2705078125
[2023-10-12 18:36:01,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=1820, skipped=31, lr=[2.743490790039882e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:36:01,381] [INFO] [timer.py:260:stop] epoch=7/micro_step=735/global_step=1820, RunningAvgSamplesPerSec=2.2194900944762246, CurrSamplesPerSec=2.467346591296182, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 7286, Loss: 0.255615234375
Epoch: 7, Total Step: 7296, Loss: 0.26611328125
Epoch: 7, Total Step: 7306, Loss: 0.296875
Epoch: 7, Total Step: 7316, Loss: 0.2138671875
[2023-10-12 18:38:10,389] [INFO] [logging.py:96:log_dist] [Rank 0] step=1830, skipped=31, lr=[2.5761200475733216e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:38:10,390] [INFO] [timer.py:260:stop] epoch=7/micro_step=775/global_step=1830, RunningAvgSamplesPerSec=2.2207741153731986, CurrSamplesPerSec=2.489917084283339, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 7326, Loss: 0.252197265625
Epoch: 7, Total Step: 7336, Loss: 0.19140625
[2023-10-12 18:39:02,027] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, but hysteresis is 2. Reducing hysteresis to 1
[2023-10-12 18:39:14,166] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
Epoch: 7, Total Step: 7346, Loss: 0.35791015625
Epoch: 7, Total Step: 7356, Loss: 0.1630859375
[2023-10-12 18:40:18,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=1840, skipped=33, lr=[2.4458143804740568e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:40:18,107] [INFO] [timer.py:260:stop] epoch=7/micro_step=815/global_step=1840, RunningAvgSamplesPerSec=2.2221548920845877, CurrSamplesPerSec=2.4809528469388105, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 7366, Loss: 0.2548828125
Epoch: 7, Total Step: 7376, Loss: 0.2275390625
Epoch: 7, Total Step: 7386, Loss: 0.216552734375
Epoch: 7, Total Step: 7396, Loss: 0.1982421875
[2023-10-12 18:42:26,147] [INFO] [logging.py:96:log_dist] [Rank 0] step=1850, skipped=33, lr=[2.2874514010315568e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:42:26,147] [INFO] [timer.py:260:stop] epoch=7/micro_step=855/global_step=1850, RunningAvgSamplesPerSec=2.223494619813817, CurrSamplesPerSec=2.646237728362628, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 7406, Loss: 0.22998046875
Epoch: 7, Total Step: 7416, Loss: 0.2252197265625
Epoch: 7, Total Step: 7426, Loss: 0.21728515625
Epoch: 7, Total Step: 7436, Loss: 0.1590576171875
[2023-10-12 18:44:32,219] [INFO] [logging.py:96:log_dist] [Rank 0] step=1860, skipped=33, lr=[2.1341424866436366e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:44:32,220] [INFO] [timer.py:260:stop] epoch=7/micro_step=895/global_step=1860, RunningAvgSamplesPerSec=2.224985406324885, CurrSamplesPerSec=2.493122491325005, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 7, Total Step: 7446, Loss: 0.2470703125
Epoch: 7, Total Step: 7456, Loss: 0.280029296875
Epoch: 7, Total Step: 7466, Loss: 0.1396484375
Epoch: 7, Total Step: 7476, Loss: 0.223876953125
[2023-10-12 18:46:38,981] [INFO] [logging.py:96:log_dist] [Rank 0] step=1870, skipped=33, lr=[1.985921752073924e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:46:38,982] [INFO] [timer.py:260:stop] epoch=7/micro_step=935/global_step=1870, RunningAvgSamplesPerSec=2.2264047628635817, CurrSamplesPerSec=2.525857364027317, MemAllocated=7.4GB, MaxMemAllocated=9.96GB
***** Evaluating perplexity, Epoch 8/9 *****
Invalidate trace cache @ step 0: expected module 0, but got module 6
ppl: 1.3895275592803955
eval loss: 0.32895952463150024
Beginning of Epoch 9/9, Total Micro Batches 935
Epoch: 8, Total Step: 7481, Loss: 0.174072265625
Epoch: 8, Total Step: 7491, Loss: 0.2354736328125
Epoch: 8, Total Step: 7501, Loss: 0.27001953125
Epoch: 8, Total Step: 7511, Loss: 0.2423095703125
[2023-10-12 18:56:04,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=1880, skipped=33, lr=[1.842822179848863e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:56:04,981] [INFO] [timer.py:260:stop] epoch=8/micro_step=40/global_step=1880, RunningAvgSamplesPerSec=2.2178117170570895, CurrSamplesPerSec=1.0907518903049427, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 7521, Loss: 0.394775390625
Epoch: 8, Total Step: 7531, Loss: 0.239501953125
Epoch: 8, Total Step: 7541, Loss: 0.25244140625
Epoch: 8, Total Step: 7551, Loss: 0.2034912109375
[2023-10-12 18:59:07,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=1890, skipped=33, lr=[1.704875612918369e-05], mom=[(0.9, 0.95)]
[2023-10-12 18:59:07,031] [INFO] [timer.py:260:stop] epoch=8/micro_step=80/global_step=1890, RunningAvgSamplesPerSec=2.214748804993185, CurrSamplesPerSec=2.491347987788513, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 7561, Loss: 0.1834716796875
Epoch: 8, Total Step: 7571, Loss: 0.38232421875
Epoch: 8, Total Step: 7581, Loss: 0.339111328125
Epoch: 8, Total Step: 7591, Loss: 0.2430419921875
[2023-10-12 19:01:16,350] [INFO] [logging.py:96:log_dist] [Rank 0] step=1900, skipped=33, lr=[1.572112747570015e-05], mom=[(0.9, 0.95)]
[2023-10-12 19:01:16,350] [INFO] [timer.py:260:stop] epoch=8/micro_step=120/global_step=1900, RunningAvgSamplesPerSec=2.2159804794294127, CurrSamplesPerSec=2.4634723382723775, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 7601, Loss: 0.239501953125
Epoch: 8, Total Step: 7611, Loss: 0.27783203125
Epoch: 8, Total Step: 7621, Loss: 0.2271728515625
Epoch: 8, Total Step: 7631, Loss: 0.2105712890625
[2023-10-12 19:03:25,689] [INFO] [logging.py:96:log_dist] [Rank 0] step=1910, skipped=33, lr=[1.4445631265984082e-05], mom=[(0.9, 0.95)]
[2023-10-12 19:03:25,690] [INFO] [timer.py:260:stop] epoch=8/micro_step=160/global_step=1910, RunningAvgSamplesPerSec=2.2171984825916797, CurrSamplesPerSec=2.467387867437812, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 7641, Loss: 0.199462890625
Epoch: 8, Total Step: 7651, Loss: 0.2406005859375
Epoch: 8, Total Step: 7661, Loss: 0.26025390625
Epoch: 8, Total Step: 7671, Loss: 0.2415771484375
[2023-10-12 19:05:35,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=1920, skipped=33, lr=[1.3222551327312398e-05], mom=[(0.9, 0.95)]
[2023-10-12 19:05:35,385] [INFO] [timer.py:260:stop] epoch=8/micro_step=200/global_step=1920, RunningAvgSamplesPerSec=2.21837640872809, CurrSamplesPerSec=2.4638923696613384, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 7681, Loss: 0.2919921875
Epoch: 8, Total Step: 7691, Loss: 0.24560546875
Epoch: 8, Total Step: 7701, Loss: 0.400634765625
Epoch: 8, Total Step: 7711, Loss: 0.1719970703125
[2023-10-12 19:07:44,960] [INFO] [logging.py:96:log_dist] [Rank 0] step=1930, skipped=33, lr=[1.2052159823134733e-05], mom=[(0.9, 0.95)]
[2023-10-12 19:07:44,961] [INFO] [timer.py:260:stop] epoch=8/micro_step=240/global_step=1930, RunningAvgSamplesPerSec=2.2195526149224545, CurrSamplesPerSec=2.4605493402299787, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 7721, Loss: 0.2548828125
Epoch: 8, Total Step: 7731, Loss: 0.297119140625
Epoch: 8, Total Step: 7741, Loss: 0.244140625
[2023-10-12 19:09:02,486] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, but hysteresis is 2. Reducing hysteresis to 1
[2023-10-12 19:09:15,388] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
Epoch: 8, Total Step: 7751, Loss: 0.2340087890625
[2023-10-12 19:09:54,234] [INFO] [logging.py:96:log_dist] [Rank 0] step=1940, skipped=35, lr=[1.1153957703109257e-05], mom=[(0.9, 0.95)]
[2023-10-12 19:09:54,234] [INFO] [timer.py:260:stop] epoch=8/micro_step=280/global_step=1940, RunningAvgSamplesPerSec=2.2207431536407163, CurrSamplesPerSec=2.4781268736397375, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 7761, Loss: 0.388671875
Epoch: 8, Total Step: 7771, Loss: 0.177001953125
Epoch: 8, Total Step: 7781, Loss: 0.233642578125
Epoch: 8, Total Step: 7791, Loss: 0.2332763671875
[2023-10-12 19:12:03,643] [INFO] [logging.py:96:log_dist] [Rank 0] step=1950, skipped=35, lr=[1.0079053771596902e-05], mom=[(0.9, 0.95)]
[2023-10-12 19:12:03,644] [INFO] [timer.py:260:stop] epoch=8/micro_step=320/global_step=1950, RunningAvgSamplesPerSec=2.2219114587834308, CurrSamplesPerSec=2.471942334955735, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 7801, Loss: 0.2939453125
Epoch: 8, Total Step: 7811, Loss: 0.2476806640625
Epoch: 8, Total Step: 7821, Loss: 0.207763671875
Epoch: 8, Total Step: 7831, Loss: 0.2142333984375
[2023-10-12 19:14:13,438] [INFO] [logging.py:96:log_dist] [Rank 0] step=1960, skipped=35, lr=[9.057537775168574e-06], mom=[(0.9, 0.95)]
[2023-10-12 19:14:13,438] [INFO] [timer.py:260:stop] epoch=8/micro_step=360/global_step=1960, RunningAvgSamplesPerSec=2.223038178943683, CurrSamplesPerSec=2.45584635722125, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 7841, Loss: 0.187744140625
Epoch: 8, Total Step: 7851, Loss: 0.22705078125
Epoch: 8, Total Step: 7861, Loss: 0.28369140625
Epoch: 8, Total Step: 7871, Loss: 0.271240234375
[2023-10-12 19:16:23,958] [INFO] [logging.py:96:log_dist] [Rank 0] step=1970, skipped=35, lr=[8.089637024655483e-06], mom=[(0.9, 0.95)]
[2023-10-12 19:16:23,958] [INFO] [timer.py:260:stop] epoch=8/micro_step=400/global_step=1970, RunningAvgSamplesPerSec=2.2240974528485844, CurrSamplesPerSec=2.4590202252615714, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 7881, Loss: 0.25048828125
Epoch: 8, Total Step: 7891, Loss: 0.291259765625
Epoch: 8, Total Step: 7901, Loss: 0.265380859375
Epoch: 8, Total Step: 7911, Loss: 0.2490234375
[2023-10-12 19:20:26,638] [INFO] [logging.py:96:log_dist] [Rank 0] step=1980, skipped=35, lr=[7.1755669002619985e-06], mom=[(0.9, 0.95)]
[2023-10-12 19:20:26,639] [INFO] [timer.py:260:stop] epoch=8/micro_step=440/global_step=1980, RunningAvgSamplesPerSec=2.216412184775568, CurrSamplesPerSec=1.4117284094430893, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 7921, Loss: 0.1951904296875
Epoch: 8, Total Step: 7931, Loss: 0.2354736328125
Epoch: 8, Total Step: 7941, Loss: 0.25634765625
Epoch: 8, Total Step: 7951, Loss: 0.2037353515625
[2023-10-12 19:24:09,586] [INFO] [logging.py:96:log_dist] [Rank 0] step=1990, skipped=35, lr=[6.31553080363867e-06], mom=[(0.9, 0.95)]
[2023-10-12 19:24:09,587] [INFO] [timer.py:260:stop] epoch=8/micro_step=480/global_step=1990, RunningAvgSamplesPerSec=2.2103691184815863, CurrSamplesPerSec=2.4627028333048195, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 7961, Loss: 0.15625
Epoch: 8, Total Step: 7971, Loss: 0.26318359375
Epoch: 8, Total Step: 7981, Loss: 0.2100830078125
Epoch: 8, Total Step: 7991, Loss: 0.319091796875
[2023-10-12 19:26:18,136] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=35, lr=[5.509720112620658e-06], mom=[(0.9, 0.95)]
[2023-10-12 19:26:18,137] [INFO] [timer.py:260:stop] epoch=8/micro_step=520/global_step=2000, RunningAvgSamplesPerSec=2.2116146025619954, CurrSamplesPerSec=2.5002583848977666, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 8001, Loss: 0.14501953125
Epoch: 8, Total Step: 8011, Loss: 0.2305908203125
Epoch: 8, Total Step: 8021, Loss: 0.2054443359375
Epoch: 8, Total Step: 8031, Loss: 0.245849609375
[2023-10-12 19:28:26,361] [INFO] [logging.py:96:log_dist] [Rank 0] step=2010, skipped=35, lr=[4.75831413864175e-06], mom=[(0.9, 0.95)]
[2023-10-12 19:28:26,362] [INFO] [timer.py:260:stop] epoch=8/micro_step=560/global_step=2010, RunningAvgSamplesPerSec=2.2128737893353785, CurrSamplesPerSec=2.4714189790439938, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 8041, Loss: 0.20361328125
Epoch: 8, Total Step: 8051, Loss: 0.1888427734375
Epoch: 8, Total Step: 8061, Loss: 0.23681640625
Epoch: 8, Total Step: 8071, Loss: 0.244140625
[2023-10-12 19:30:34,967] [INFO] [logging.py:96:log_dist] [Rank 0] step=2020, skipped=35, lr=[4.0614800868334075e-06], mom=[(0.9, 0.95)]
[2023-10-12 19:30:34,968] [INFO] [timer.py:260:stop] epoch=8/micro_step=600/global_step=2020, RunningAvgSamplesPerSec=2.2140934421781013, CurrSamplesPerSec=2.4937285622480636, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 8081, Loss: 0.310791015625
Epoch: 8, Total Step: 8091, Loss: 0.2078857421875
Epoch: 8, Total Step: 8101, Loss: 0.2449951171875
Epoch: 8, Total Step: 8111, Loss: 0.287353515625
[2023-10-12 19:32:43,475] [INFO] [logging.py:96:log_dist] [Rank 0] step=2030, skipped=35, lr=[3.4193730188178675e-06], mom=[(0.9, 0.95)]
[2023-10-12 19:32:43,475] [INFO] [timer.py:260:stop] epoch=8/micro_step=640/global_step=2030, RunningAvgSamplesPerSec=2.2153095453795113, CurrSamplesPerSec=2.4910221000755652, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 8121, Loss: 0.18408203125
Epoch: 8, Total Step: 8131, Loss: 0.34619140625
Epoch: 8, Total Step: 8141, Loss: 0.256103515625
Epoch: 8, Total Step: 8151, Loss: 0.182861328125
[2023-10-12 19:34:26,478] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, but hysteresis is 2. Reducing hysteresis to 1
[2023-10-12 19:34:39,320] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072
[2023-10-12 19:34:52,142] [INFO] [logging.py:96:log_dist] [Rank 0] step=2040, skipped=37, lr=[2.945187220527945e-06], mom=[(0.9, 0.95)]
[2023-10-12 19:34:52,143] [INFO] [timer.py:260:stop] epoch=8/micro_step=680/global_step=2040, RunningAvgSamplesPerSec=2.2165030650276516, CurrSamplesPerSec=2.4969646074111544, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 8161, Loss: 0.236328125
Epoch: 8, Total Step: 8171, Loss: 0.26708984375
Epoch: 8, Total Step: 8181, Loss: 0.325439453125
Epoch: 8, Total Step: 8191, Loss: 0.2021484375
[2023-10-12 19:37:01,428] [INFO] [logging.py:96:log_dist] [Rank 0] step=2050, skipped=37, lr=[2.401940586039236e-06], mom=[(0.9, 0.95)]
[2023-10-12 19:37:01,429] [INFO] [timer.py:260:stop] epoch=8/micro_step=720/global_step=2050, RunningAvgSamplesPerSec=2.2176399364403334, CurrSamplesPerSec=2.449959443193757, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 8201, Loss: 0.285400390625
Epoch: 8, Total Step: 8211, Loss: 0.26953125
Epoch: 8, Total Step: 8221, Loss: 0.25341796875
Epoch: 8, Total Step: 8231, Loss: 0.26513671875
[2023-10-12 19:39:11,578] [INFO] [logging.py:96:log_dist] [Rank 0] step=2060, skipped=37, lr=[1.9137902210956683e-06], mom=[(0.9, 0.95)]
[2023-10-12 19:39:11,578] [INFO] [timer.py:260:stop] epoch=8/micro_step=760/global_step=2060, RunningAvgSamplesPerSec=2.2187021487599816, CurrSamplesPerSec=2.4807307703745756, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 8241, Loss: 0.295166015625
Epoch: 8, Total Step: 8251, Loss: 0.2122802734375
Epoch: 8, Total Step: 8261, Loss: 0.25
Epoch: 8, Total Step: 8271, Loss: 0.1910400390625
[2023-10-12 19:41:20,864] [INFO] [logging.py:96:log_dist] [Rank 0] step=2070, skipped=37, lr=[1.4808447503938949e-06], mom=[(0.9, 0.95)]
[2023-10-12 19:41:20,865] [INFO] [timer.py:260:stop] epoch=8/micro_step=800/global_step=2070, RunningAvgSamplesPerSec=2.219819711043837, CurrSamplesPerSec=2.4743421474338736, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 8281, Loss: 0.35546875
Epoch: 8, Total Step: 8291, Loss: 0.162353515625
Epoch: 8, Total Step: 8301, Loss: 0.25341796875
Epoch: 8, Total Step: 8311, Loss: 0.2265625
[2023-10-12 19:44:24,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=2080, skipped=37, lr=[1.1032005142703194e-06], mom=[(0.9, 0.95)]
[2023-10-12 19:44:24,264] [INFO] [timer.py:260:stop] epoch=8/micro_step=840/global_step=2080, RunningAvgSamplesPerSec=2.2169221379471824, CurrSamplesPerSec=1.4501112500059288, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 8321, Loss: 0.21533203125
Epoch: 8, Total Step: 8331, Loss: 0.1964111328125
Epoch: 8, Total Step: 8341, Loss: 0.2283935546875
Epoch: 8, Total Step: 8351, Loss: 0.22314453125
[2023-10-12 19:48:46,143] [INFO] [logging.py:96:log_dist] [Rank 0] step=2090, skipped=37, lr=[7.809415472633807e-07], mom=[(0.9, 0.95)]
[2023-10-12 19:48:46,144] [INFO] [timer.py:260:stop] epoch=8/micro_step=880/global_step=2090, RunningAvgSamplesPerSec=2.20831783544841, CurrSamplesPerSec=1.3114369103063002, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 8361, Loss: 0.2164306640625
Epoch: 8, Total Step: 8371, Loss: 0.1583251953125
Epoch: 8, Total Step: 8381, Loss: 0.2442626953125
Epoch: 8, Total Step: 8391, Loss: 0.276123046875
[2023-10-12 19:51:18,312] [INFO] [logging.py:96:log_dist] [Rank 0] step=2100, skipped=37, lr=[5.141395594136788e-07], mom=[(0.9, 0.95)]
[2023-10-12 19:51:18,312] [INFO] [timer.py:260:stop] epoch=8/micro_step=920/global_step=2100, RunningAvgSamplesPerSec=2.2077958156705835, CurrSamplesPerSec=2.401185148588414, MemAllocated=7.52GB, MaxMemAllocated=9.96GB
Epoch: 8, Total Step: 8401, Loss: 0.1395263671875
Epoch: 8, Total Step: 8411, Loss: 0.22265625
***** Evaluating perplexity, Epoch 9/9 *****
Invalidate trace cache @ step 0: expected module 0, but got module 6
ppl: 1.3883421421051025
eval loss: 0.3281061053276062
saving the final model ...
[2023-10-12 19:57:02,673] [INFO] [launch.py:347:main] Process 3366574 exits successfully.
[2023-10-12 19:57:36,710] [INFO] [launch.py:347:main] Process 3366573 exits successfully.
